{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pythag(pf,pa,exp=11.5):\n",
    "    return (pf**exp)/(pa**exp+pf**exp)\n",
    "\n",
    "def pythagGame(df_game,exp=11.5):\n",
    "    p={}\n",
    "    for iteam in np.arange(1,3):\n",
    "        pf = df_game['AdjO_'+str(iteam)]\n",
    "        pa = df_game['AdjD_'+str(iteam)]\n",
    "        p[str(iteam)] = pythag(pf,pa,exp)\n",
    "    \n",
    "    return p['1']*(1.-p['2'])/(p['1']+p['2']-2.*p['1']*p['2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teamType=pd.read_csv('data/team_type.csv')\n",
    "teamTypeDict={}\n",
    "for r in teamType.values:\n",
    "    teamTypeDict[r[0]]=r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/games/all_games_2017.csv',index_col=0)\n",
    "# df=pd.read_csv('data/games/tourn_games_2016.csv',index_col=0)\n",
    "# y=df['outcome']\n",
    "dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1', 'name_1',\\\n",
    "            'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'name_2', 'outcome']\n",
    "\n",
    "y=df['outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types=[]\n",
    "for r in df.itertuples():\n",
    "    \n",
    "    t1=teamTypeDict[r.School_1]\n",
    "    t2=teamTypeDict[r.School_2]\n",
    "    \n",
    "    types.append(str(min(t1,t2))+str(max(t1,t2)))\n",
    "    \n",
    "df['types']=types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=[]\n",
    "dfAll=pd.DataFrame()\n",
    "for year in np.arange(2017,2003,-1):\n",
    "    df=pd.read_csv('data/games/tourn_games_'+str(year)+'.csv',index_col=0)\n",
    "    dfAll=dfAll.append(df,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1', 'name_1',\\\n",
    "            'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'name_2', 'outcome', 'region']\n",
    "dropLabels.append('round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.839\n",
      "Accuracy using pythagorean win expectation: 0.733\n"
     ]
    }
   ],
   "source": [
    "X=dfAll.drop(dropLabels,axis=1).dropna(axis=1)\n",
    "y=dfAll['outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "# model = LogisticRegression(C=0.001,multi_class='multinomial',solver='sag')\n",
    "model = LogisticRegression(C=0.001)\n",
    "# model=RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=5, random_state=0)\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean())) \n",
    "\n",
    "y_predPythag=pythagGame(X_test)\n",
    "\n",
    "print('Accuracy using pythagorean win expectation: %.3f' % ((np.round(y_predPythag)==y_test).sum()/len(y_test)))\n",
    "\n",
    "res.append([year,results.mean(),(np.round(y_predPythag)==y_test).sum()/len(y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, 44)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.75662122e-05],\n",
       "       [  1.30242487e-05],\n",
       "       [  1.82790561e-05],\n",
       "       [  7.36844603e-06],\n",
       "       [  1.28372954e-05],\n",
       "       [  1.58261539e-05],\n",
       "       [  1.54875395e-05],\n",
       "       [  3.80490767e-06],\n",
       "       [  1.96280284e-05],\n",
       "       [  6.80287069e-06],\n",
       "       [  6.13525201e-07],\n",
       "       [  1.92596776e-05],\n",
       "       [  1.94567205e-05],\n",
       "       [  1.34820584e-05],\n",
       "       [  1.96224848e-05],\n",
       "       [  1.94388599e-05],\n",
       "       [  1.96022060e-05],\n",
       "       [  9.50411855e-06],\n",
       "       [  1.15587826e-06],\n",
       "       [  5.49725060e-06],\n",
       "       [  5.32529601e-06],\n",
       "       [  1.62985769e-05],\n",
       "       [  1.96847116e-07],\n",
       "       [  1.66791097e-05],\n",
       "       [  6.70547615e-06],\n",
       "       [  1.78087547e-05],\n",
       "       [  4.67560823e-06],\n",
       "       [  9.20250151e-06],\n",
       "       [  1.92078993e-05],\n",
       "       [  1.65670761e-05],\n",
       "       [  1.96689697e-05],\n",
       "       [  2.29510627e-07],\n",
       "       [  2.78670419e-06],\n",
       "       [  1.81252777e-05],\n",
       "       [  1.36513567e-05],\n",
       "       [  1.88587331e-05],\n",
       "       [  1.93750864e-05],\n",
       "       [  7.14179064e-06],\n",
       "       [  1.00715067e-05],\n",
       "       [  1.92023858e-05],\n",
       "       [  9.24035628e-07],\n",
       "       [  6.11613309e-06],\n",
       "       [  4.89690838e-06],\n",
       "       [  1.72353591e-05],\n",
       "       [  1.87128755e-05],\n",
       "       [  1.09896473e-05],\n",
       "       [  1.95957193e-05],\n",
       "       [  1.48337485e-05],\n",
       "       [  1.81890247e-05],\n",
       "       [  3.44184264e-06],\n",
       "       [  1.63771179e-05],\n",
       "       [  1.55086488e-05],\n",
       "       [  1.10703353e-05],\n",
       "       [  1.61010886e-05],\n",
       "       [  1.56515535e-05],\n",
       "       [  1.86068004e-05],\n",
       "       [  1.13807712e-05],\n",
       "       [  5.67461523e-06],\n",
       "       [  5.88475465e-06],\n",
       "       [  1.82006044e-05],\n",
       "       [  1.80156222e-05],\n",
       "       [  1.34482011e-05],\n",
       "       [  1.03599694e-05],\n",
       "       [  1.65278176e-05],\n",
       "       [  1.76519552e-05],\n",
       "       [  1.09802159e-06],\n",
       "       [  1.81580115e-05],\n",
       "       [  1.96371482e-05],\n",
       "       [  1.19859922e-05],\n",
       "       [  9.62419202e-08],\n",
       "       [  1.96900888e-05],\n",
       "       [  1.91147133e-05],\n",
       "       [  1.14645067e-05],\n",
       "       [  3.88991723e-06],\n",
       "       [  1.82084434e-05],\n",
       "       [  1.69620562e-05],\n",
       "       [  1.51753746e-05],\n",
       "       [  4.90403766e-06],\n",
       "       [  1.88738965e-05],\n",
       "       [  1.87346852e-05],\n",
       "       [  1.96917918e-05],\n",
       "       [  1.52518542e-05],\n",
       "       [  2.00388647e-07],\n",
       "       [  7.58191178e-06],\n",
       "       [  1.26297343e-05],\n",
       "       [  6.64055719e-06],\n",
       "       [  1.54350925e-05],\n",
       "       [  1.66655399e-06],\n",
       "       [  1.34746554e-05],\n",
       "       [  8.77270794e-06],\n",
       "       [  3.92473786e-06],\n",
       "       [  2.69892369e-07],\n",
       "       [  8.70589704e-06],\n",
       "       [  4.56416645e-06],\n",
       "       [  1.10028512e-05],\n",
       "       [  6.93086274e-06],\n",
       "       [  9.95963438e-06],\n",
       "       [  9.59242594e-06],\n",
       "       [  3.57502413e-07],\n",
       "       [  1.95246801e-05],\n",
       "       [  1.05514236e-05],\n",
       "       [  1.30627225e-06],\n",
       "       [  1.72624418e-05],\n",
       "       [  1.92961347e-05],\n",
       "       [  4.03597555e-06],\n",
       "       [  1.48705185e-05],\n",
       "       [  1.08394346e-05],\n",
       "       [  1.04501636e-06],\n",
       "       [  4.97954255e-07],\n",
       "       [  4.96451587e-06],\n",
       "       [  9.70135289e-06],\n",
       "       [  1.96395283e-05],\n",
       "       [  5.70812033e-06],\n",
       "       [  8.42267552e-06],\n",
       "       [  5.29789357e-06],\n",
       "       [  1.96366785e-05],\n",
       "       [  7.55772746e-07],\n",
       "       [  5.07815658e-06],\n",
       "       [  1.74759202e-05],\n",
       "       [  7.39127460e-06],\n",
       "       [  1.50425502e-05],\n",
       "       [  8.22514571e-06],\n",
       "       [  1.83073602e-05],\n",
       "       [  1.68885204e-05],\n",
       "       [  3.37698888e-06],\n",
       "       [  1.69517669e-05],\n",
       "       [  7.39978256e-06],\n",
       "       [  1.12956174e-05],\n",
       "       [  1.95151682e-05],\n",
       "       [  9.89648929e-06],\n",
       "       [  1.31365620e-06],\n",
       "       [  1.38276747e-05],\n",
       "       [  1.56199286e-05],\n",
       "       [  7.26564004e-06],\n",
       "       [  1.92170936e-05],\n",
       "       [  1.23617231e-05],\n",
       "       [  1.54600755e-05],\n",
       "       [  1.89378066e-05],\n",
       "       [  2.96296693e-06],\n",
       "       [  1.47432172e-07],\n",
       "       [  1.13310848e-06],\n",
       "       [  2.04930675e-06],\n",
       "       [  1.87134301e-05],\n",
       "       [  1.65780959e-05],\n",
       "       [  1.93169327e-05],\n",
       "       [  2.36987891e-06],\n",
       "       [  5.07473605e-06],\n",
       "       [  1.95852429e-05],\n",
       "       [  3.34274388e-06],\n",
       "       [  1.32210031e-05],\n",
       "       [  1.86209224e-06],\n",
       "       [  1.96902738e-05],\n",
       "       [  1.66180589e-05],\n",
       "       [  1.32019684e-05],\n",
       "       [  3.61491628e-07],\n",
       "       [  1.59595417e-06],\n",
       "       [  3.43446989e-06],\n",
       "       [  1.28686739e-05],\n",
       "       [  1.28821446e-05],\n",
       "       [  1.17698562e-05],\n",
       "       [  1.73039193e-05],\n",
       "       [  1.90167784e-05],\n",
       "       [  5.76288400e-06],\n",
       "       [  5.77021868e-06],\n",
       "       [  2.13302094e-07],\n",
       "       [  1.32549207e-05],\n",
       "       [  1.67293357e-06],\n",
       "       [  7.14284419e-06],\n",
       "       [  6.64107876e-06],\n",
       "       [  9.12885007e-06],\n",
       "       [  1.43814854e-05],\n",
       "       [  1.40925602e-05],\n",
       "       [  2.34007290e-06],\n",
       "       [  3.63832461e-07],\n",
       "       [  1.89603372e-05],\n",
       "       [  1.27814011e-05],\n",
       "       [  1.35295651e-05],\n",
       "       [  4.15065368e-06],\n",
       "       [  1.68198358e-05],\n",
       "       [  1.79711974e-05],\n",
       "       [  1.85268722e-05],\n",
       "       [  7.23720700e-06],\n",
       "       [  1.51443228e-05],\n",
       "       [  1.96337481e-05],\n",
       "       [  4.40100248e-06],\n",
       "       [  1.09618267e-05],\n",
       "       [  3.89128972e-07],\n",
       "       [  5.30106918e-06],\n",
       "       [  7.23522584e-07],\n",
       "       [  7.99472417e-06],\n",
       "       [  3.78482725e-07],\n",
       "       [  1.39240719e-05],\n",
       "       [  1.96010568e-05],\n",
       "       [  1.46474434e-05],\n",
       "       [  1.90261216e-05],\n",
       "       [  9.61988626e-06],\n",
       "       [  1.70508866e-05],\n",
       "       [  1.44989261e-07],\n",
       "       [  1.31523229e-05],\n",
       "       [  6.91946639e-06],\n",
       "       [  6.42505764e-06],\n",
       "       [  1.87824414e-05],\n",
       "       [  1.71603974e-05],\n",
       "       [  7.70432302e-06],\n",
       "       [  1.29378151e-05],\n",
       "       [  1.06044399e-05],\n",
       "       [  2.50726008e-06],\n",
       "       [  1.48825726e-05],\n",
       "       [  1.17021622e-05],\n",
       "       [  1.90504857e-05],\n",
       "       [  1.92974694e-05],\n",
       "       [  1.58563825e-05],\n",
       "       [  1.34518355e-05],\n",
       "       [  1.94613314e-05],\n",
       "       [  8.37017904e-06],\n",
       "       [  1.27816289e-05],\n",
       "       [  1.40406658e-06],\n",
       "       [  7.25498553e-06],\n",
       "       [  1.94288575e-05],\n",
       "       [  2.18070068e-06],\n",
       "       [  1.83230120e-05]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logit(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "logit(X_test.dot(model.coef_.T)).values-model.predict_proba(X_test)[:,1,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.33573159,  0.66426841],\n",
       "       [ 1.        ,  0.20906013,  0.79093987],\n",
       "       [ 1.        ,  0.3660799 ,  0.6339201 ],\n",
       "       [ 1.        ,  0.10446177,  0.89553823],\n",
       "       [ 1.        ,  0.20500935,  0.79499065],\n",
       "       [ 1.        ,  0.27847231,  0.72152769],\n",
       "       [ 1.        ,  0.26897328,  0.73102672],\n",
       "       [ 0.        ,  0.94910606,  0.05089394],\n",
       "       [ 0.        ,  0.52848918,  0.47151082],\n",
       "       [ 1.        ,  0.09548677,  0.90451323],\n",
       "       [ 1.        ,  0.00785099,  0.99214901],\n",
       "       [ 0.        ,  0.57408727,  0.42591273],\n",
       "       [ 1.        ,  0.44536611,  0.55463389],\n",
       "       [ 0.        ,  0.78078716,  0.21921284],\n",
       "       [ 0.        ,  0.52969911,  0.47030089],\n",
       "       [ 0.        ,  0.5566902 ,  0.4433098 ],\n",
       "       [ 1.        ,  0.46626213,  0.53373787],\n",
       "       [ 0.        ,  0.85964323,  0.14035677],\n",
       "       [ 1.        ,  0.01489701,  0.98510299],\n",
       "       [ 1.        ,  0.0754921 ,  0.9245079 ],\n",
       "       [ 0.        ,  0.92707684,  0.07292316],\n",
       "       [ 1.        ,  0.2924501 ,  0.7075499 ],\n",
       "       [ 1.        ,  0.00250546,  0.99749454],\n",
       "       [ 1.        ,  0.30443414,  0.69556586],\n",
       "       [ 1.        ,  0.09396125,  0.90603875],\n",
       "       [ 1.        ,  0.34538748,  0.65461252],\n",
       "       [ 1.        ,  0.06337835,  0.93662165],\n",
       "       [ 1.        ,  0.13508127,  0.86491873],\n",
       "       [ 0.        ,  0.57839871,  0.42160129],\n",
       "       [ 1.        ,  0.30083098,  0.69916902],\n",
       "       [ 0.        ,  0.5170774 ,  0.4829226 ],\n",
       "       [ 1.        ,  0.00292242,  0.99707758],\n",
       "       [ 0.        ,  0.96327368,  0.03672632],\n",
       "       [ 1.        ,  0.35897947,  0.64102053],\n",
       "       [ 0.        ,  0.77693336,  0.22306664],\n",
       "       [ 1.        ,  0.39716141,  0.60283859],\n",
       "       [ 0.        ,  0.56343143,  0.43656857],\n",
       "       [ 1.        ,  0.10084079,  0.89915921],\n",
       "       [ 1.        ,  0.15052467,  0.84947533],\n",
       "       [ 0.        ,  0.57884391,  0.42115609],\n",
       "       [ 1.        ,  0.01187256,  0.98812744],\n",
       "       [ 1.        ,  0.0848498 ,  0.9151502 ],\n",
       "       [ 1.        ,  0.06660773,  0.93339227],\n",
       "       [ 1.        ,  0.32340917,  0.67659083],\n",
       "       [ 1.        ,  0.38852187,  0.61147813],\n",
       "       [ 0.        ,  0.83239113,  0.16760887],\n",
       "       [ 1.        ,  0.46506331,  0.53493669],\n",
       "       [ 1.        ,  0.25165853,  0.74834147],\n",
       "       [ 0.        ,  0.63813962,  0.36186038],\n",
       "       [ 1.        ,  0.04579487,  0.95420513],\n",
       "       [ 1.        ,  0.29486623,  0.70513377],\n",
       "       [ 1.        ,  0.26955401,  0.73044599],\n",
       "       [ 0.        ,  0.83084663,  0.16915337],\n",
       "       [ 1.        ,  0.28649559,  0.71350441],\n",
       "       [ 1.        ,  0.27352455,  0.72647545],\n",
       "       [ 1.        ,  0.38263749,  0.61736251],\n",
       "       [ 1.        ,  0.17517547,  0.82482453],\n",
       "       [ 1.        ,  0.07815269,  0.92184731],\n",
       "       [ 1.        ,  0.0813268 ,  0.9186732 ],\n",
       "       [ 1.        ,  0.36241175,  0.63758825],\n",
       "       [ 0.        ,  0.64589076,  0.35410924],\n",
       "       [ 0.        ,  0.78155153,  0.21844847],\n",
       "       [ 1.        ,  0.15580418,  0.84419582],\n",
       "       [ 0.        ,  0.70043283,  0.29956717],\n",
       "       [ 1.        ,  0.3390789 ,  0.6609211 ],\n",
       "       [ 1.        ,  0.01414049,  0.98585951],\n",
       "       [ 0.        ,  0.63955753,  0.36044247],\n",
       "       [ 0.        ,  0.52637824,  0.47362176],\n",
       "       [ 1.        ,  0.18722644,  0.81277356],\n",
       "       [ 1.        ,  0.00122339,  0.99877661],\n",
       "       [ 1.        ,  0.49519499,  0.50480501],\n",
       "       [ 1.        ,  0.4144064 ,  0.5855936 ],\n",
       "       [ 1.        ,  0.176816  ,  0.823184  ],\n",
       "       [ 1.        ,  0.05210095,  0.94789905],\n",
       "       [ 0.        ,  0.63724434,  0.36275566],\n",
       "       [ 0.        ,  0.68617266,  0.31382734],\n",
       "       [ 1.        ,  0.2605498 ,  0.7394502 ],\n",
       "       [ 0.        ,  0.93329273,  0.06670727],\n",
       "       [ 0.        ,  0.60191727,  0.39808273],\n",
       "       [ 1.        ,  0.38977066,  0.61022934],\n",
       "       [ 0.        ,  0.50125961,  0.49874039],\n",
       "       [ 1.        ,  0.26258589,  0.73741411],\n",
       "       [ 1.        ,  0.00255065,  0.99744935],\n",
       "       [ 1.        ,  0.1079026 ,  0.8920974 ],\n",
       "       [ 1.        ,  0.20057623,  0.79942377],\n",
       "       [ 0.        ,  0.90705905,  0.09294095],\n",
       "       [ 1.        ,  0.26753671,  0.73246329],\n",
       "       [ 1.        ,  0.02162634,  0.97837366],\n",
       "       [ 1.        ,  0.21905901,  0.78094099],\n",
       "       [ 1.        ,  0.12767992,  0.87232008],\n",
       "       [ 1.        ,  0.05259473,  0.94740527],\n",
       "       [ 1.        ,  0.00343839,  0.99656161],\n",
       "       [ 1.        ,  0.12654256,  0.87345744],\n",
       "       [ 0.        ,  0.93824345,  0.06175655],\n",
       "       [ 1.        ,  0.16787213,  0.83212787],\n",
       "       [ 0.        ,  0.9025066 ,  0.0974934 ],\n",
       "       [ 1.        ,  0.1484985 ,  0.8515015 ],\n",
       "       [ 1.        ,  0.14192837,  0.85807163],\n",
       "       [ 1.        ,  0.00455966,  0.99544034],\n",
       "       [ 1.        ,  0.45393222,  0.54606778],\n",
       "       [ 1.        ,  0.15935339,  0.84064661],\n",
       "       [ 1.        ,  0.01686906,  0.98313094],\n",
       "       [ 1.        ,  0.32438536,  0.67561464],\n",
       "       [ 1.        ,  0.42912486,  0.57087514],\n",
       "       [ 1.        ,  0.05417581,  0.94582419],\n",
       "       [ 0.        ,  0.7474147 ,  0.2525853 ],\n",
       "       [ 1.        ,  0.16476335,  0.83523665],\n",
       "       [ 1.        ,  0.01344844,  0.98655156],\n",
       "       [ 1.        ,  0.00636254,  0.99363746],\n",
       "       [ 1.        ,  0.06759913,  0.93240087],\n",
       "       [ 1.        ,  0.14386466,  0.85613534],\n",
       "       [ 0.        ,  0.52579891,  0.47420109],\n",
       "       [ 1.        ,  0.07865717,  0.92134283],\n",
       "       [ 0.        ,  0.87824933,  0.12175067],\n",
       "       [ 0.        ,  0.92748393,  0.07251607],\n",
       "       [ 1.        ,  0.47352856,  0.52647144],\n",
       "       [ 1.        ,  0.00968921,  0.99031079],\n",
       "       [ 1.        ,  0.06927069,  0.93072931],\n",
       "       [ 1.        ,  0.33227888,  0.66772112],\n",
       "       [ 1.        ,  0.10482831,  0.89517169],\n",
       "       [ 0.        ,  0.74296084,  0.25703916],\n",
       "       [ 0.        ,  0.88154981,  0.11845019],\n",
       "       [ 1.        ,  0.36742821,  0.63257179],\n",
       "       [ 0.        ,  0.68866337,  0.31133663],\n",
       "       [ 1.        ,  0.04488938,  0.95511062],\n",
       "       [ 1.        ,  0.31349379,  0.68650621],\n",
       "       [ 1.        ,  0.104965  ,  0.895035  ],\n",
       "       [ 0.        ,  0.82649569,  0.17350431],\n",
       "       [ 1.        ,  0.45263994,  0.54736006],\n",
       "       [ 0.        ,  0.85264991,  0.14735009],\n",
       "       [ 1.        ,  0.01696609,  0.98303391],\n",
       "       [ 1.        ,  0.22715192,  0.77284808],\n",
       "       [ 1.        ,  0.27263989,  0.72736011],\n",
       "       [ 1.        ,  0.10281528,  0.89718472],\n",
       "       [ 0.        ,  0.57765061,  0.42234939],\n",
       "       [ 1.        ,  0.19494728,  0.80505272],\n",
       "       [ 0.        ,  0.73179555,  0.26820445],\n",
       "       [ 0.        ,  0.59785543,  0.40214457],\n",
       "       [ 1.        ,  0.03915068,  0.96084932],\n",
       "       [ 1.        ,  0.00187533,  0.99812467],\n",
       "       [ 1.        ,  0.01459914,  0.98540086],\n",
       "       [ 1.        ,  0.02673272,  0.97326728],\n",
       "       [ 1.        ,  0.38855346,  0.61144654],\n",
       "       [ 0.        ,  0.69883408,  0.30116592],\n",
       "       [ 1.        ,  0.43101248,  0.56898752],\n",
       "       [ 1.        ,  0.03105231,  0.96894769],\n",
       "       [ 1.        ,  0.06922028,  0.93077972],\n",
       "       [ 0.        ,  0.53681007,  0.46318993],\n",
       "       [ 1.        ,  0.04441197,  0.95558803],\n",
       "       [ 0.        ,  0.78662811,  0.21337189],\n",
       "       [ 1.        ,  0.02422821,  0.97577179],\n",
       "       [ 0.        ,  0.50457425,  0.49542575],\n",
       "       [ 0.        ,  0.6975541 ,  0.3024459 ],\n",
       "       [ 0.        ,  0.78704935,  0.21295065],\n",
       "       [ 1.        ,  0.00461078,  0.99538922],\n",
       "       [ 1.        ,  0.0206904 ,  0.9793096 ],\n",
       "       [ 1.        ,  0.04569184,  0.95430816],\n",
       "       [ 1.        ,  0.20568534,  0.79431466],\n",
       "       [ 0.        ,  0.79403686,  0.20596314],\n",
       "       [ 1.        ,  0.18287024,  0.81712976],\n",
       "       [ 1.        ,  0.32589101,  0.67410899],\n",
       "       [ 1.        ,  0.40742852,  0.59257148],\n",
       "       [ 1.        ,  0.07948305,  0.92051695],\n",
       "       [ 1.        ,  0.07959379,  0.92040621],\n",
       "       [ 1.        ,  0.00271547,  0.99728453],\n",
       "       [ 1.        ,  0.21413728,  0.78586272],\n",
       "       [ 0.        ,  0.97829067,  0.02170933],\n",
       "       [ 1.        ,  0.10085754,  0.89914246],\n",
       "       [ 1.        ,  0.09295572,  0.90704428],\n",
       "       [ 1.        ,  0.13380232,  0.86619768],\n",
       "       [ 0.        ,  0.7596587 ,  0.2403413 ],\n",
       "       [ 0.        ,  0.76662846,  0.23337154],\n",
       "       [ 1.        ,  0.03064901,  0.96935099],\n",
       "       [ 1.        ,  0.00464078,  0.99535922],\n",
       "       [ 0.        ,  0.59638267,  0.40361733],\n",
       "       [ 0.        ,  0.79620376,  0.20379624],\n",
       "       [ 1.        ,  0.22030243,  0.77969757],\n",
       "       [ 1.        ,  0.05581168,  0.94418832],\n",
       "       [ 0.        ,  0.69096042,  0.30903958],\n",
       "       [ 1.        ,  0.35220682,  0.64779318],\n",
       "       [ 0.        ,  0.62162701,  0.37837299],\n",
       "       [ 1.        ,  0.10236111,  0.89763889],\n",
       "       [ 0.        ,  0.74028711,  0.25971289],\n",
       "       [ 0.        ,  0.52718444,  0.47281556],\n",
       "       [ 1.        ,  0.05940399,  0.94059601],\n",
       "       [ 0.        ,  0.832922  ,  0.167078  ],\n",
       "       [ 1.        ,  0.00496506,  0.99503494],\n",
       "       [ 1.        ,  0.07256853,  0.92743147],\n",
       "       [ 1.        ,  0.00927185,  0.99072815],\n",
       "       [ 0.        ,  0.88536416,  0.11463584],\n",
       "       [ 1.        ,  0.00482855,  0.99517145],\n",
       "       [ 0.        ,  0.77061005,  0.22938995],\n",
       "       [ 0.        ,  0.53397294,  0.46602706],\n",
       "       [ 1.        ,  0.24694129,  0.75305871],\n",
       "       [ 0.        ,  0.59194766,  0.40805234],\n",
       "       [ 1.        ,  0.14241552,  0.85758448],\n",
       "       [ 0.        ,  0.68311873,  0.31688127],\n",
       "       [ 1.        ,  0.00184419,  0.99815581],\n",
       "       [ 0.        ,  0.78814512,  0.21185488],\n",
       "       [ 1.        ,  0.09732063,  0.90267937],\n",
       "       [ 1.        ,  0.08960066,  0.91039934],\n",
       "       [ 0.        ,  0.60746301,  0.39253699],\n",
       "       [ 0.        ,  0.67928225,  0.32071775],\n",
       "       [ 0.        ,  0.89011827,  0.10988173],\n",
       "       [ 1.        ,  0.20718038,  0.79281962],\n",
       "       [ 0.        ,  0.83966783,  0.16033217],\n",
       "       [ 0.        ,  0.96708681,  0.03291319],\n",
       "       [ 1.        ,  0.25290964,  0.74709036],\n",
       "       [ 0.        ,  0.81849357,  0.18150643],\n",
       "       [ 1.        ,  0.40976925,  0.59023075],\n",
       "       [ 0.        ,  0.57077481,  0.42922519],\n",
       "       [ 1.        ,  0.27934018,  0.72065982],\n",
       "       [ 1.        ,  0.21854387,  0.78145613],\n",
       "       [ 1.        ,  0.44590439,  0.55409561],\n",
       "       [ 1.        ,  0.12087908,  0.87912092],\n",
       "       [ 1.        ,  0.2038139 ,  0.7961861 ],\n",
       "       [ 0.        ,  0.98184568,  0.01815432],\n",
       "       [ 0.        ,  0.89736222,  0.10263778],\n",
       "       [ 1.        ,  0.44221991,  0.55778009],\n",
       "       [ 1.        ,  0.02849842,  0.97150158],\n",
       "       [ 0.        ,  0.63183858,  0.36816142]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(y_pred[:,None],model.predict_proba(X_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School_1</th>\n",
       "      <th>School_2</th>\n",
       "      <th>wpct_1</th>\n",
       "      <th>wpct_2</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>villanova</td>\n",
       "      <td>mount-st.-mary's</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>virginia-tech</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginia</td>\n",
       "      <td>north-carolina-wilmington</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>florida</td>\n",
       "      <td>east-tennessee-state</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>southern-methodist</td>\n",
       "      <td>southern-california</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baylor</td>\n",
       "      <td>new-mexico-state</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>south-carolina</td>\n",
       "      <td>marquette</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.594</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>duke</td>\n",
       "      <td>troy</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kansas</td>\n",
       "      <td>uc-davis</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>miami-(fl)</td>\n",
       "      <td>michigan-state</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>iowa-state</td>\n",
       "      <td>nevada</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>purdue</td>\n",
       "      <td>vermont</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>creighton</td>\n",
       "      <td>rhode-island</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>oregon</td>\n",
       "      <td>iona</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.629</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>michigan</td>\n",
       "      <td>oklahoma-state</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>louisville</td>\n",
       "      <td>jacksonville-state</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>north-carolina</td>\n",
       "      <td>texas-southern</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>arkansas</td>\n",
       "      <td>seton-hall</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>minnesota</td>\n",
       "      <td>middle-tennessee</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>butler</td>\n",
       "      <td>winthrop</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.788</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cincinnati</td>\n",
       "      <td>kansas-state</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ucla</td>\n",
       "      <td>kent-state</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dayton</td>\n",
       "      <td>wichita-state</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>kentucky</td>\n",
       "      <td>northern-kentucky</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.686</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>gonzaga</td>\n",
       "      <td>south-dakota-state</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>northwestern</td>\n",
       "      <td>vanderbilt</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>notre-dame</td>\n",
       "      <td>princeton</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>west-virginia</td>\n",
       "      <td>bucknell</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>maryland</td>\n",
       "      <td>xavier</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>florida-state</td>\n",
       "      <td>florida-gulf-coast</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>providence</td>\n",
       "      <td>pacific</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>kansas</td>\n",
       "      <td>illinois-chicago</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>boston-college</td>\n",
       "      <td>utah</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.727</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>georgia-tech</td>\n",
       "      <td>northern-iowa</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>michigan-state</td>\n",
       "      <td>nevada</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>gonzaga</td>\n",
       "      <td>valparaiso</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>duke</td>\n",
       "      <td>alabama-state</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>seton-hall</td>\n",
       "      <td>arizona</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>illinois</td>\n",
       "      <td>murray-state</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>cincinnati</td>\n",
       "      <td>east-tennessee-state</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>north-carolina</td>\n",
       "      <td>air-force</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>texas</td>\n",
       "      <td>princeton</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>xavier</td>\n",
       "      <td>louisville</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>mississippi-state</td>\n",
       "      <td>monmouth</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>saint-joseph's</td>\n",
       "      <td>liberty</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>texas-tech</td>\n",
       "      <td>charlotte</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>florida</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>wake-forest</td>\n",
       "      <td>virginia-commonwealth</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>richmond</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>central-florida</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.806</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>memphis</td>\n",
       "      <td>south-carolina</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>oklahoma-state</td>\n",
       "      <td>eastern-washington</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.567</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>stanford</td>\n",
       "      <td>texas-san-antonio</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>alabama</td>\n",
       "      <td>southern-illinois</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>syracuse</td>\n",
       "      <td>brigham-young</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>maryland</td>\n",
       "      <td>texas-el-paso</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>vanderbilt</td>\n",
       "      <td>western-michigan</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>north-carolina-state</td>\n",
       "      <td>louisiana-lafayette</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>depaul</td>\n",
       "      <td>dayton</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.727</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>connecticut</td>\n",
       "      <td>vermont</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 School_1                   School_2  wpct_1  wpct_2  outcome\n",
       "0               villanova           mount-st.-mary's   0.889   0.556        1\n",
       "1               wisconsin              virginia-tech   0.730   0.667        1\n",
       "2                virginia  north-carolina-wilmington   0.676   0.829        1\n",
       "3                 florida       east-tennessee-state   0.750   0.771        1\n",
       "4      southern-methodist        southern-california   0.857   0.722        0\n",
       "5                  baylor           new-mexico-state   0.771   0.824        1\n",
       "6          south-carolina                  marquette   0.703   0.594        1\n",
       "7                    duke                       troy   0.757   0.595        1\n",
       "15                 kansas                   uc-davis   0.861   0.639        1\n",
       "16             miami-(fl)             michigan-state   0.636   0.571        0\n",
       "17             iowa-state                     nevada   0.686   0.800        1\n",
       "18                 purdue                    vermont   0.771   0.829        1\n",
       "19              creighton               rhode-island   0.714   0.714        0\n",
       "20                 oregon                       iona   0.846   0.629        1\n",
       "21               michigan             oklahoma-state   0.684   0.606        1\n",
       "22             louisville         jacksonville-state   0.735   0.571        1\n",
       "30         north-carolina             texas-southern   0.825   0.657        1\n",
       "31               arkansas                 seton-hall   0.722   0.636        1\n",
       "32              minnesota           middle-tennessee   0.706   0.861        0\n",
       "33                 butler                   winthrop   0.735   0.788        1\n",
       "34             cincinnati               kansas-state   0.833   0.600        1\n",
       "35                   ucla                 kent-state   0.861   0.611        1\n",
       "36                 dayton              wichita-state   0.750   0.861        0\n",
       "37               kentucky          northern-kentucky   0.842   0.686        1\n",
       "45                gonzaga         south-dakota-state   0.949   0.514        1\n",
       "46           northwestern                 vanderbilt   0.667   0.543        1\n",
       "47             notre-dame                  princeton   0.722   0.767        1\n",
       "48          west-virginia                   bucknell   0.757   0.743        1\n",
       "49               maryland                     xavier   0.727   0.632        0\n",
       "50          florida-state         florida-gulf-coast   0.743   0.765        1\n",
       "..                    ...                        ...     ...     ...      ...\n",
       "821            providence                    pacific   0.690   0.758        0\n",
       "822                kansas           illinois-chicago   0.727   0.750        1\n",
       "823        boston-college                       utah   0.706   0.727        1\n",
       "824          georgia-tech              northern-iowa   0.737   0.677        1\n",
       "825        michigan-state                     nevada   0.600   0.735        0\n",
       "826               gonzaga                 valparaiso   0.903   0.581        1\n",
       "834                  duke              alabama-state   0.838   0.516        1\n",
       "835            seton-hall                    arizona   0.677   0.667        1\n",
       "836              illinois               murray-state   0.788   0.824        1\n",
       "837            cincinnati       east-tennessee-state   0.781   0.818        1\n",
       "838        north-carolina                  air-force   0.633   0.759        1\n",
       "839                 texas                  princeton   0.758   0.714        1\n",
       "840                xavier                 louisville   0.703   0.667        1\n",
       "841     mississippi-state                   monmouth   0.867   0.636        1\n",
       "849        saint-joseph's                    liberty   0.938   0.545        1\n",
       "850            texas-tech                  charlotte   0.676   0.700        1\n",
       "851               florida                  manhattan   0.645   0.806        0\n",
       "852           wake-forest      virginia-commonwealth   0.677   0.742        1\n",
       "853             wisconsin                   richmond   0.781   0.606        1\n",
       "854            pittsburgh            central-florida   0.861   0.806        1\n",
       "855               memphis             south-carolina   0.733   0.676        1\n",
       "856        oklahoma-state         eastern-washington   0.886   0.567        1\n",
       "864              stanford          texas-san-antonio   0.938   0.576        1\n",
       "865               alabama          southern-illinois   0.606   0.833        1\n",
       "866              syracuse              brigham-young   0.742   0.700        1\n",
       "867              maryland              texas-el-paso   0.625   0.750        1\n",
       "868            vanderbilt           western-michigan   0.697   0.839        1\n",
       "869  north-carolina-state        louisiana-lafayette   0.677   0.690        1\n",
       "870                depaul                     dayton   0.688   0.727        1\n",
       "871           connecticut                    vermont   0.846   0.710        1\n",
       "\n",
       "[448 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll[dfAll['round']==1][['School_1','School_2','wpct_1','wpct_2','outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43,  37],\n",
       "       [ 22, 119]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,np.round(y_predPythag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411764"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(22+43)/(37+119.+22+43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8144796380090498"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(18+23)/(57+123.+18+23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year       2010.500000\n",
       "Log reg       0.743571\n",
       "Pythag        0.721805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resDF.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3],\n",
       "       [ 2, 12]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73684210526315785"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(X_train,y_train)\n",
    "pred=rf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhr9f32T7aBysfLGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee236d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resDF.plot(x='Log reg',y='Pythag',kind='scatter')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 613\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.68515497553\n",
      "10-fold cross validation average accuracy: 0.669\n",
      "01 281\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy using pythagorean win expectation:  0.79359430605\n",
      "10-fold cross validation average accuracy: 0.830\n",
      "02 940\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.76170212766\n",
      "10-fold cross validation average accuracy: 0.754\n",
      "11 388\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.682989690722\n",
      "10-fold cross validation average accuracy: 0.669\n",
      "12 287\n",
      "Accuracy of logistic regression classifier on test set: 0.92\n",
      "Accuracy using pythagorean win expectation:  0.91637630662\n",
      "10-fold cross validation average accuracy: 0.922\n",
      "22 530\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.694339622642\n",
      "10-fold cross validation average accuracy: 0.687\n",
      "\n",
      "0.737416863787\n",
      "0.742020401448\n",
      "00 573\n",
      "Accuracy of logistic regression classifier on test set: 0.72\n",
      "Accuracy using pythagorean win expectation:  0.713787085515\n",
      "10-fold cross validation average accuracy: 0.651\n",
      "01 282\n",
      "Accuracy of logistic regression classifier on test set: 0.81\n",
      "Accuracy using pythagorean win expectation:  0.808510638298\n",
      "10-fold cross validation average accuracy: 0.815\n",
      "02 863\n",
      "Accuracy of logistic regression classifier on test set: 0.78\n",
      "Accuracy using pythagorean win expectation:  0.800695249131\n",
      "10-fold cross validation average accuracy: 0.778\n",
      "11 408\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.671568627451\n",
      "10-fold cross validation average accuracy: 0.670\n",
      "12 380\n",
      "Accuracy of logistic regression classifier on test set: 0.91\n",
      "Accuracy using pythagorean win expectation:  0.910526315789\n",
      "10-fold cross validation average accuracy: 0.920\n",
      "22 526\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.665399239544\n",
      "10-fold cross validation average accuracy: 0.642\n",
      "\n",
      "0.737129063522\n",
      "0.757915567282\n",
      "00 309\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.669902912621\n",
      "10-fold cross validation average accuracy: 0.630\n",
      "01 840\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.789285714286\n",
      "10-fold cross validation average accuracy: 0.771\n",
      "02 282\n",
      "Accuracy of logistic regression classifier on test set: 0.91\n",
      "Accuracy using pythagorean win expectation:  0.918439716312\n",
      "10-fold cross validation average accuracy: 0.899\n",
      "11 644\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.680124223602\n",
      "10-fold cross validation average accuracy: 0.662\n",
      "12 390\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.776923076923\n",
      "10-fold cross validation average accuracy: 0.774\n",
      "22 536\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.67723880597\n",
      "10-fold cross validation average accuracy: 0.693\n",
      "\n",
      "0.731521485249\n",
      "0.744085304898\n",
      "00 586\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.692832764505\n",
      "10-fold cross validation average accuracy: 0.699\n",
      "01 287\n",
      "Accuracy of logistic regression classifier on test set: 0.82\n",
      "Accuracy using pythagorean win expectation:  0.794425087108\n",
      "10-fold cross validation average accuracy: 0.801\n",
      "02 442\n",
      "Accuracy of logistic regression classifier on test set: 0.85\n",
      "Accuracy using pythagorean win expectation:  0.864253393665\n",
      "10-fold cross validation average accuracy: 0.867\n",
      "11 345\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.649275362319\n",
      "10-fold cross validation average accuracy: 0.644\n",
      "12 802\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.779301745636\n",
      "10-fold cross validation average accuracy: 0.788\n",
      "22 543\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.664825046041\n",
      "10-fold cross validation average accuracy: 0.681\n",
      "\n",
      "0.74747637304\n",
      "0.740765391015\n",
      "00 532\n",
      "Accuracy of logistic regression classifier on test set: 0.66\n",
      "Accuracy using pythagorean win expectation:  0.65037593985\n",
      "10-fold cross validation average accuracy: 0.698\n",
      "01 341\n",
      "Accuracy of logistic regression classifier on test set: 0.91\n",
      "Accuracy using pythagorean win expectation:  0.91788856305\n",
      "10-fold cross validation average accuracy: 0.913\n",
      "02 296\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.787162162162\n",
      "10-fold cross validation average accuracy: 0.790\n",
      "11 532\n",
      "Accuracy of logistic regression classifier on test set: 0.66\n",
      "Accuracy using pythagorean win expectation:  0.646616541353\n",
      "10-fold cross validation average accuracy: 0.685\n",
      "12 810\n",
      "Accuracy of logistic regression classifier on test set: 0.80\n",
      "Accuracy using pythagorean win expectation:  0.802469135802\n",
      "10-fold cross validation average accuracy: 0.783\n",
      "22 471\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.709129511677\n",
      "10-fold cross validation average accuracy: 0.658\n",
      "\n",
      "0.746160913262\n",
      "0.744466800805\n",
      "00 536\n",
      "Accuracy of logistic regression classifier on test set: 0.73\n",
      "Accuracy using pythagorean win expectation:  0.722014925373\n",
      "10-fold cross validation average accuracy: 0.694\n",
      "01 715\n",
      "Accuracy of logistic regression classifier on test set: 0.75\n",
      "Accuracy using pythagorean win expectation:  0.74965034965\n",
      "10-fold cross validation average accuracy: 0.770\n",
      "02 513\n",
      "Accuracy of logistic regression classifier on test set: 0.90\n",
      "Accuracy using pythagorean win expectation:  0.906432748538\n",
      "10-fold cross validation average accuracy: 0.913\n",
      "11 293\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.686006825939\n",
      "10-fold cross validation average accuracy: 0.691\n",
      "12 322\n",
      "Accuracy of logistic regression classifier on test set: 0.74\n",
      "Accuracy using pythagorean win expectation:  0.754658385093\n",
      "10-fold cross validation average accuracy: 0.761\n",
      "22 557\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.678635547576\n",
      "10-fold cross validation average accuracy: 0.701\n",
      "\n",
      "0.759121806828\n",
      "0.75272479564\n",
      "00 411\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.698296836983\n",
      "10-fold cross validation average accuracy: 0.662\n",
      "01 787\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.790343074968\n",
      "10-fold cross validation average accuracy: 0.779\n",
      "02 237\n",
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "Accuracy using pythagorean win expectation:  0.940928270042\n",
      "10-fold cross validation average accuracy: 0.936\n",
      "11 663\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.613876319759\n",
      "10-fold cross validation average accuracy: 0.661\n",
      "12 468\n",
      "Accuracy of logistic regression classifier on test set: 0.80\n",
      "Accuracy using pythagorean win expectation:  0.801282051282\n",
      "10-fold cross validation average accuracy: 0.812\n",
      "22 357\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.627450980392\n",
      "10-fold cross validation average accuracy: 0.670\n",
      "\n",
      "0.740584422896\n",
      "0.731440301061\n",
      "00 360\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.716666666667\n",
      "10-fold cross validation average accuracy: 0.652\n",
      "01 184\n",
      "Accuracy of logistic regression classifier on test set: 0.98\n",
      "Accuracy using pythagorean win expectation:  0.994565217391\n",
      "10-fold cross validation average accuracy: 0.960\n",
      "02 501\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy using pythagorean win expectation:  0.834331337325\n",
      "10-fold cross validation average accuracy: 0.823\n",
      "11 368\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.627\n",
      "12 638\n",
      "Accuracy of logistic regression classifier on test set: 0.82\n",
      "Accuracy using pythagorean win expectation:  0.815047021944\n",
      "10-fold cross validation average accuracy: 0.799\n",
      "22 674\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.695845697329\n",
      "10-fold cross validation average accuracy: 0.650\n",
      "\n",
      "0.734948243054\n",
      "0.771009174312\n",
      "00 612\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.62091503268\n",
      "10-fold cross validation average accuracy: 0.644\n",
      "01 431\n",
      "Accuracy of logistic regression classifier on test set: 0.88\n",
      "Accuracy using pythagorean win expectation:  0.867749419954\n",
      "10-fold cross validation average accuracy: 0.845\n",
      "02 646\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.78173374613\n",
      "10-fold cross validation average accuracy: 0.764\n",
      "11 406\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.709359605911\n",
      "10-fold cross validation average accuracy: 0.654\n",
      "12 195\n",
      "Accuracy of logistic regression classifier on test set: 0.93\n",
      "Accuracy using pythagorean win expectation:  0.94358974359\n",
      "10-fold cross validation average accuracy: 0.960\n",
      "22 404\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.660891089109\n",
      "10-fold cross validation average accuracy: 0.656\n",
      "\n",
      "0.731065819635\n",
      "0.741648106904\n",
      "00 499\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.689378757515\n",
      "10-fold cross validation average accuracy: 0.688\n",
      "01 337\n",
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "Accuracy using pythagorean win expectation:  0.756676557864\n",
      "10-fold cross validation average accuracy: 0.761\n",
      "02 312\n",
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "Accuracy using pythagorean win expectation:  0.942307692308\n",
      "10-fold cross validation average accuracy: 0.937\n",
      "11 362\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.657458563536\n",
      "10-fold cross validation average accuracy: 0.610\n",
      "12 701\n",
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "Accuracy using pythagorean win expectation:  0.760342368046\n",
      "10-fold cross validation average accuracy: 0.757\n",
      "22 450\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.673333333333\n",
      "10-fold cross validation average accuracy: 0.656\n",
      "\n",
      "0.728378703589\n",
      "0.739195791056\n",
      "00 326\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.684049079755\n",
      "10-fold cross validation average accuracy: 0.612\n",
      "01 655\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.795419847328\n",
      "10-fold cross validation average accuracy: 0.757\n",
      "02 208\n",
      "Accuracy of logistic regression classifier on test set: 0.93\n",
      "Accuracy using pythagorean win expectation:  0.932692307692\n",
      "10-fold cross validation average accuracy: 0.928\n",
      "11 533\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.679174484053\n",
      "10-fold cross validation average accuracy: 0.683\n",
      "12 416\n",
      "Accuracy of logistic regression classifier on test set: 0.78\n",
      "Accuracy using pythagorean win expectation:  0.762019230769\n",
      "10-fold cross validation average accuracy: 0.803\n",
      "22 481\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.656964656965\n",
      "10-fold cross validation average accuracy: 0.703\n",
      "\n",
      "0.734598731354\n",
      "0.738067964872\n",
      "00 482\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.676348547718\n",
      "10-fold cross validation average accuracy: 0.661\n",
      "01 517\n",
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "Accuracy using pythagorean win expectation:  0.762088974855\n",
      "10-fold cross validation average accuracy: 0.779\n",
      "02 351\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.786324786325\n",
      "10-fold cross validation average accuracy: 0.766\n",
      "11 355\n",
      "Accuracy of logistic regression classifier on test set: 0.73\n",
      "Accuracy using pythagorean win expectation:  0.732394366197\n",
      "10-fold cross validation average accuracy: 0.683\n",
      "12 280\n",
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "Accuracy using pythagorean win expectation:  0.946428571429\n",
      "10-fold cross validation average accuracy: 0.925\n",
      "22 530\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.705660377358\n",
      "10-fold cross validation average accuracy: 0.670\n",
      "\n",
      "0.734261502166\n",
      "0.753479125249\n",
      "00 571\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.661996497373\n",
      "10-fold cross validation average accuracy: 0.625\n",
      "01 518\n",
      "Accuracy of logistic regression classifier on test set: 0.75\n",
      "Accuracy using pythagorean win expectation:  0.764478764479\n",
      "10-fold cross validation average accuracy: 0.774\n",
      "02 562\n",
      "Accuracy of logistic regression classifier on test set: 0.83\n",
      "Accuracy using pythagorean win expectation:  0.834519572954\n",
      "10-fold cross validation average accuracy: 0.825\n",
      "11 185\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.708108108108\n",
      "10-fold cross validation average accuracy: 0.674\n",
      "12 139\n",
      "Accuracy of logistic regression classifier on test set: 0.96\n",
      "Accuracy using pythagorean win expectation:  0.956834532374\n",
      "10-fold cross validation average accuracy: 0.932\n",
      "22 537\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.683426443203\n",
      "10-fold cross validation average accuracy: 0.673\n",
      "\n",
      "0.731158697519\n",
      "0.74601910828\n",
      "00 259\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.644787644788\n",
      "10-fold cross validation average accuracy: 0.669\n",
      "01 581\n",
      "Accuracy of logistic regression classifier on test set: 0.81\n",
      "Accuracy using pythagorean win expectation:  0.822719449225\n",
      "10-fold cross validation average accuracy: 0.837\n",
      "02 95\n",
      "Accuracy of logistic regression classifier on test set: 0.96\n",
      "Accuracy using pythagorean win expectation:  0.989473684211\n",
      "10-fold cross validation average accuracy: 0.918\n",
      "11 706\n",
      "Accuracy of logistic regression classifier on test set: 0.66\n",
      "Accuracy using pythagorean win expectation:  0.661473087819\n",
      "10-fold cross validation average accuracy: 0.687\n",
      "12 437\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy using pythagorean win expectation:  0.844393592677\n",
      "10-fold cross validation average accuracy: 0.815\n",
      "22 415\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.698795180723\n",
      "10-fold cross validation average accuracy: 0.707\n",
      "\n",
      "0.754569502728\n",
      "0.748094665062\n"
     ]
    }
   ],
   "source": [
    "resultList=[]\n",
    "for yr in np.arange(2017,2003,-1):\n",
    "    year=str(yr)\n",
    "    teamType=pd.read_csv('data/team_type_'+year+'.csv')\n",
    "    teamTypeDict={}\n",
    "    for r in teamType.values:\n",
    "        teamTypeDict[r[0]]=r[1]\n",
    "\n",
    "    df=pd.read_csv('data/games/all_games_'+year+'.csv',index_col=0).dropna(axis=1)\n",
    "    # df=pd.read_csv('data/games/tourn_games_2016.csv',index_col=0)\n",
    "    # y=df['outcome']\n",
    "    dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1',\\\n",
    "                'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'outcome']\n",
    "\n",
    "    y=df['outcome']\n",
    "\n",
    "\n",
    "    types=[]\n",
    "    for r in df.itertuples():\n",
    "\n",
    "        t1=teamTypeDict[r.School_1]\n",
    "        t2=teamTypeDict[r.School_2]\n",
    "\n",
    "        types.append(str(min(t1,t2))+str(max(t1,t2)))\n",
    "\n",
    "    df['types']=types\n",
    "\n",
    "    runsum=0\n",
    "    numsum=0\n",
    "    pythsum=0\n",
    "\n",
    "    nlab=3\n",
    "    for t1lab in range(nlab):\n",
    "        for t2lab in np.arange(t1lab,nlab):\n",
    "            lab = str(t1lab)+str(t2lab)\n",
    "\n",
    "            dfSel = df[(df.types==lab)]\n",
    "            dropLabels.append('types')\n",
    "\n",
    "            X=dfSel.drop(dropLabels,axis=1)\n",
    "            y=dfSel['outcome']\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X_train, y_train)\n",
    "\n",
    "            y_pred=logreg.predict(X_test)\n",
    "            print(lab,len(y_pred))\n",
    "            print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "            y_predPythag=pythagGame(X_test)\n",
    "\n",
    "            print('Accuracy using pythagorean win expectation: ',(np.round(y_predPythag)==y_test).sum()/len(y_test))\n",
    "\n",
    "\n",
    "            kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "            modelCV = LogisticRegression()\n",
    "            scoring = 'accuracy'\n",
    "            results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "            print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))    \n",
    "\n",
    "            runsum+= len(y_pred) * results.mean() #logreg.score(X_test, y_test)\n",
    "            numsum+= len(y_pred)\n",
    "            pythsum+= len(y_pred) *(np.round(y_predPythag)==y_test).sum()/len(y_test)\n",
    "\n",
    "\n",
    "\n",
    "    print ()\n",
    "    print (runsum/numsum)\n",
    "    print (pythsum/numsum)\n",
    "    \n",
    "    resultList.append([year,(runsum/numsum),pythsum/numsum])\n",
    "    \n",
    "resDFKM=pd.DataFrame(resultList,columns=['year','Log reg','Pythag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year       1.440858e+54\n",
       "Log reg    7.391709e-01\n",
       "Pythag     7.464952e-01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resDFKM.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.70\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "# logreg = LogisticRegression(C=1,penalty='l1',tol=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred=logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pythag']=pythagGame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using pythagorean win expectation:  0.739459815547\n"
     ]
    }
   ],
   "source": [
    "y_predPythag=pythagGame(X_test)\n",
    "\n",
    "print('Accuracy using pythagorean win expectation: ',(np.round(y_predPythag)==y_test).sum()/len(y_test))\n",
    "# print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
