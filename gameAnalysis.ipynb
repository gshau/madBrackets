{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:14:00.264607Z",
     "start_time": "2018-01-18T05:13:58.488169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:14:00.283729Z",
     "start_time": "2018-01-18T05:14:00.267324Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pythag(pf,pa,exp=11.5):\n",
    "    return (pf**exp)/(pa**exp+pf**exp)\n",
    "\n",
    "def pythagGame(df_game,exp=11.5):\n",
    "    p={}\n",
    "    for iteam in np.arange(1,3):\n",
    "        pf = df_game['AdjO_'+str(iteam)]\n",
    "        pa = df_game['AdjD_'+str(iteam)]\n",
    "        p[str(iteam)] = pythag(pf,pa,exp)\n",
    "    \n",
    "    return p['1']*(1.-p['2'])/(p['1']+p['2']-2.*p['1']*p['2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:14:00.369807Z",
     "start_time": "2018-01-18T05:14:00.360555Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teamType=pd.read_csv('data/team_type.csv')\n",
    "teamTypeDict={}\n",
    "for r in teamType.values:\n",
    "    teamTypeDict[r[0]]=r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:14:01.006506Z",
     "start_time": "2018-01-18T05:14:01.000011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df=pd.read_csv('data/games/all_games_2017.csv',index_col=0)\n",
    "# # df=pd.read_csv('data/games/tourn_games_2016.csv',index_col=0)\n",
    "# # y=df['outcome']\n",
    "# # dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1', 'name_1',\\\n",
    "# #             'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'name_2', 'outcome']\n",
    "\n",
    "# # y=df['outcome']\n",
    "# dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1', 'wpct_1','TmPts_1', 'OppPts_1',\\\n",
    "#             'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'wpct_2','TmPts_2', 'OppPts_2', 'outcome']\n",
    "# # dropLabels.append('round')\n",
    "# dfAll=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:14:11.794827Z",
     "start_time": "2018-01-18T05:14:11.790403Z"
    }
   },
   "outputs": [],
   "source": [
    "# types=[]\n",
    "# for r in df.itertuples():\n",
    "    \n",
    "#     t1=teamTypeDict[r.School_1]\n",
    "#     t2=teamTypeDict[r.School_2]\n",
    "    \n",
    "#     types.append(str(min(t1,t2))+str(max(t1,t2)))\n",
    "    \n",
    "# df['types']=types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:53:22.053945Z",
     "start_time": "2018-01-18T05:53:21.838883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=[]\n",
    "dfAll=pd.DataFrame()\n",
    "for year in np.arange(2017,2003,-1):\n",
    "    df=pd.read_csv('data/games/tourn_games_'+str(year)+'.csv',index_col=0)\n",
    "    dfAll=dfAll.append(df,ignore_index=True)\n",
    "\n",
    "\n",
    "df2=dfAll.copy()\n",
    "index=df2.keys()\n",
    "index\n",
    "df2.columns=[indx.replace('_1','_xx').replace('_2','_1').replace('_xx','_2') for indx in index]\n",
    "df2.outcome=1-df2.outcome\n",
    "\n",
    "dfAll=(df2.append(dfAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:53:22.384479Z",
     "start_time": "2018-01-18T05:53:22.378620Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1', 'name_1', 'wpct_1','TmPts_1', 'OppPts_1',\\\n",
    "            'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'name_2', 'wpct_2','TmPts_2', 'OppPts_2', 'outcome', 'region']\n",
    "dropLabels.append('round')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on scaled inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T19:03:18.161614Z",
     "start_time": "2018-01-18T19:03:17.995299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dfClean = dfAll.drop(dropLabels,axis=1).dropna(axis=1)\n",
    "scaler=StandardScaler().fit(dfClean)\n",
    "\n",
    "scaled=scaler.transform(dfClean)\n",
    "\n",
    "\n",
    "X=scaled\n",
    "y=dfAll['outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# model = LogisticRegression(C=0.001,multi_class='multinomial',solver='sag')\n",
    "model = LogisticRegression()\n",
    "# model=RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=5, random_state=0)\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean())) \n",
    "\n",
    "# y_predPythag=pythagGame(X_test)\n",
    "\n",
    "# print('Accuracy using pythagorean win expectation: %.3f' % ((np.round(y_predPythag)==y_test).sum()/len(y_test)))\n",
    "\n",
    "# res.append([year,results.mean(),(np.round(y_predPythag)==y_test).sum()/len(y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T19:04:24.783506Z",
     "start_time": "2018-01-18T19:04:24.775441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,'scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on non-scaled inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T19:05:52.063367Z",
     "start_time": "2018-01-18T19:05:52.031028Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropLabels=['outcome', 'region']\n",
    "dropLabels.append('round')\n",
    "for t12 in ['1', '2']:\n",
    "    dropLabels.append('School_'+t12)\n",
    "    dropLabels.append('name_'+t12)\n",
    "    dropLabels.append('fullName_'+t12)\n",
    "    dropLabels.append('Conf_'+t12)\n",
    "    dropLabels.append('wpct_'+t12)\n",
    "    dropLabels.append('Rank_'+t12)\n",
    "    dropLabels.append('WL_'+t12)\n",
    "    dropLabels.append('sched_url_'+t12)\n",
    "    dropLabels.append('name_'+t12)\n",
    "    dropLabels.append('TmPts_'+t12)\n",
    "    dropLabels.append('OppPts_'+t12)\n",
    "    dropLabels.append('SRS_'+t12)\n",
    "    dropLabels.append('SOS_'+t12)\n",
    "#     dropLabels.append('AdjO_'+t12)\n",
    "#     dropLabels.append('AdjD_'+t12)\n",
    "#     dropLabels.append('AdjT_'+t12)\n",
    "#     dropLabels.append('Luck_'+t12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T19:05:52.198645Z",
     "start_time": "2018-01-18T19:05:52.168279Z"
    }
   },
   "outputs": [],
   "source": [
    "dfClean=dfAll.drop(dropLabels,axis=1).dropna(axis=1)\n",
    "\n",
    "\n",
    "features12=dfClean.keys()\n",
    "features=[l.replace('_1','') for l in features12[::2]]\n",
    "dfNew=pd.DataFrame()\n",
    "for lab in features:\n",
    "    dfNew[lab]=dfClean[lab+'_1']-dfClean[lab+'_2']\n",
    "\n",
    "\n",
    "dfClean=dfNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T19:11:55.062171Z",
     "start_time": "2018-01-18T19:11:54.913825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.764\n"
     ]
    }
   ],
   "source": [
    "n_fold = 10\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "cols=dfClean.columns\n",
    "dfClean=dfClean[sorted(cols)]\n",
    "\n",
    "scaler=StandardScaler().fit(dfClean)\n",
    "scaled=scaler.transform(dfClean)\n",
    "# X=scaled\n",
    "X=dfClean\n",
    "\n",
    "y=dfAll['outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "# model = LogisticRegression(C=0.001,multi_class='multinomial',solver='sag')\n",
    "model = LogisticRegression(C=0.01,penalty='l2',fit_intercept=True)\n",
    "# model=RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "kfold = model_selection.KFold(n_splits=n_fold, random_state=0)\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean())) \n",
    "\n",
    "\n",
    "# y_predPythag=pythagGame(X_test)\n",
    "\n",
    "# print('Accuracy using pythagorean win expectation: %.3f' % ((np.round(y_predPythag)==y_test).sum()/len(y_test)))\n",
    "\n",
    "# res.append([year,results.mean(),(np.round(y_predPythag)==y_test).sum()/len(y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T19:11:57.087204Z",
     "start_time": "2018-01-18T19:11:57.071872Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'app/model.pkl') \n",
    "\n",
    "features=dfClean.columns\n",
    "joblib.dump(features,'app/features.pkl')\n",
    "\n",
    "clf = joblib.load('app/model.pkl') \n",
    "featuresX=joblib.load('app/features.pkl')\n",
    "\n",
    "# joblib.dump(scaler,'app/scaler.pkl')\n",
    "pickle.dump(scaler, open('app/scaler.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:52:00.868003Z",
     "start_time": "2018-01-18T05:52:00.859710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['southern-california' 'baylor']\n",
      "[ 0.9035948  0.0964052]\n"
     ]
    }
   ],
   "source": [
    "row=10\n",
    "print(dfAll.loc[row][['name_1','name_2']].values)\n",
    "print(clf.predict_proba(dfClean)[row,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write model to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T02:15:38.323032Z",
     "start_time": "2018-01-18T02:15:38.281339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://gshau@localhost/ncaabb\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "dbname = 'ncaabb'\n",
    "username = 'gshau' # change this to your username\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T02:22:54.501798Z",
     "start_time": "2018-01-18T02:22:54.447618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3PAr_1</th>\n",
       "      <th>3PAr_2</th>\n",
       "      <th>AST%_1</th>\n",
       "      <th>AST%_2</th>\n",
       "      <th>AdjD_1</th>\n",
       "      <th>AdjD_2</th>\n",
       "      <th>AdjEM.1_1</th>\n",
       "      <th>AdjEM.1_2</th>\n",
       "      <th>AdjEM_1</th>\n",
       "      <th>AdjEM_2</th>\n",
       "      <th>...</th>\n",
       "      <th>STL%_1</th>\n",
       "      <th>STL%_2</th>\n",
       "      <th>TOV%_1</th>\n",
       "      <th>TOV%_2</th>\n",
       "      <th>TRB%_1</th>\n",
       "      <th>TRB%_2</th>\n",
       "      <th>TS%_1</th>\n",
       "      <th>TS%_2</th>\n",
       "      <th>eFG%_1</th>\n",
       "      <th>eFG%_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.369</td>\n",
       "      <td>0.433</td>\n",
       "      <td>51.9</td>\n",
       "      <td>52.3</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>22.99</td>\n",
       "      <td>29.88</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.7</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.315</td>\n",
       "      <td>0.313</td>\n",
       "      <td>52.3</td>\n",
       "      <td>49.3</td>\n",
       "      <td>104.4</td>\n",
       "      <td>95.5</td>\n",
       "      <td>-4.80</td>\n",
       "      <td>8.82</td>\n",
       "      <td>5.07</td>\n",
       "      <td>20.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7</td>\n",
       "      <td>10.2</td>\n",
       "      <td>15.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>53.9</td>\n",
       "      <td>52.7</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.376</td>\n",
       "      <td>51.6</td>\n",
       "      <td>63.7</td>\n",
       "      <td>91.4</td>\n",
       "      <td>100.4</td>\n",
       "      <td>10.98</td>\n",
       "      <td>5.17</td>\n",
       "      <td>27.72</td>\n",
       "      <td>23.63</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>53.2</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.383</td>\n",
       "      <td>0.393</td>\n",
       "      <td>65.3</td>\n",
       "      <td>53.6</td>\n",
       "      <td>93.9</td>\n",
       "      <td>96.3</td>\n",
       "      <td>9.00</td>\n",
       "      <td>14.05</td>\n",
       "      <td>23.12</td>\n",
       "      <td>23.45</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>54.9</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.433</td>\n",
       "      <td>0.369</td>\n",
       "      <td>52.3</td>\n",
       "      <td>46.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>103.4</td>\n",
       "      <td>9.33</td>\n",
       "      <td>-4.49</td>\n",
       "      <td>29.88</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.5</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.381</td>\n",
       "      <td>0.364</td>\n",
       "      <td>61.0</td>\n",
       "      <td>59.1</td>\n",
       "      <td>96.3</td>\n",
       "      <td>92.6</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1.50</td>\n",
       "      <td>15.57</td>\n",
       "      <td>26.14</td>\n",
       "      <td>...</td>\n",
       "      <td>11.2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>49.7</td>\n",
       "      <td>56.1</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.406</td>\n",
       "      <td>57.3</td>\n",
       "      <td>57.8</td>\n",
       "      <td>89.4</td>\n",
       "      <td>98.4</td>\n",
       "      <td>10.84</td>\n",
       "      <td>10.24</td>\n",
       "      <td>27.17</td>\n",
       "      <td>19.74</td>\n",
       "      <td>...</td>\n",
       "      <td>13.8</td>\n",
       "      <td>10.4</td>\n",
       "      <td>14.1</td>\n",
       "      <td>12.3</td>\n",
       "      <td>52.1</td>\n",
       "      <td>48.7</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.388</td>\n",
       "      <td>0.451</td>\n",
       "      <td>56.3</td>\n",
       "      <td>52.6</td>\n",
       "      <td>93.2</td>\n",
       "      <td>99.2</td>\n",
       "      <td>8.05</td>\n",
       "      <td>10.76</td>\n",
       "      <td>25.02</td>\n",
       "      <td>23.05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>12.8</td>\n",
       "      <td>52.9</td>\n",
       "      <td>47.9</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.403</td>\n",
       "      <td>0.383</td>\n",
       "      <td>49.3</td>\n",
       "      <td>47.5</td>\n",
       "      <td>107.6</td>\n",
       "      <td>96.8</td>\n",
       "      <td>-2.86</td>\n",
       "      <td>11.43</td>\n",
       "      <td>2.04</td>\n",
       "      <td>24.17</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>51.8</td>\n",
       "      <td>51.9</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.411</td>\n",
       "      <td>0.361</td>\n",
       "      <td>57.1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>99.0</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.73</td>\n",
       "      <td>14.34</td>\n",
       "      <td>16.70</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>16.3</td>\n",
       "      <td>15.7</td>\n",
       "      <td>50.3</td>\n",
       "      <td>54.4</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.328</td>\n",
       "      <td>0.366</td>\n",
       "      <td>59.1</td>\n",
       "      <td>56.4</td>\n",
       "      <td>92.9</td>\n",
       "      <td>100.7</td>\n",
       "      <td>13.08</td>\n",
       "      <td>5.49</td>\n",
       "      <td>24.23</td>\n",
       "      <td>13.45</td>\n",
       "      <td>...</td>\n",
       "      <td>7.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>17.1</td>\n",
       "      <td>13.7</td>\n",
       "      <td>56.5</td>\n",
       "      <td>50.3</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.433</td>\n",
       "      <td>0.369</td>\n",
       "      <td>52.3</td>\n",
       "      <td>51.9</td>\n",
       "      <td>92.5</td>\n",
       "      <td>92.1</td>\n",
       "      <td>9.33</td>\n",
       "      <td>10.00</td>\n",
       "      <td>29.88</td>\n",
       "      <td>22.99</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.7</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.376</td>\n",
       "      <td>0.349</td>\n",
       "      <td>63.7</td>\n",
       "      <td>45.5</td>\n",
       "      <td>100.4</td>\n",
       "      <td>103.3</td>\n",
       "      <td>5.17</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>23.63</td>\n",
       "      <td>1.68</td>\n",
       "      <td>...</td>\n",
       "      <td>7.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>13.4</td>\n",
       "      <td>14.9</td>\n",
       "      <td>52.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.384</td>\n",
       "      <td>0.325</td>\n",
       "      <td>54.1</td>\n",
       "      <td>57.3</td>\n",
       "      <td>100.3</td>\n",
       "      <td>89.4</td>\n",
       "      <td>-3.62</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.38</td>\n",
       "      <td>27.17</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.1</td>\n",
       "      <td>50.6</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.359</td>\n",
       "      <td>0.380</td>\n",
       "      <td>54.5</td>\n",
       "      <td>65.3</td>\n",
       "      <td>94.5</td>\n",
       "      <td>95.9</td>\n",
       "      <td>12.43</td>\n",
       "      <td>10.99</td>\n",
       "      <td>27.45</td>\n",
       "      <td>15.51</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>15.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.411</td>\n",
       "      <td>0.330</td>\n",
       "      <td>52.7</td>\n",
       "      <td>56.4</td>\n",
       "      <td>105.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>11.68</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.29</td>\n",
       "      <td>...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>9.1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>13.7</td>\n",
       "      <td>51.6</td>\n",
       "      <td>52.3</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.422</td>\n",
       "      <td>51.6</td>\n",
       "      <td>52.4</td>\n",
       "      <td>91.4</td>\n",
       "      <td>104.9</td>\n",
       "      <td>10.98</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>27.72</td>\n",
       "      <td>1.91</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>13.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>53.2</td>\n",
       "      <td>53.1</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.451</td>\n",
       "      <td>0.357</td>\n",
       "      <td>52.6</td>\n",
       "      <td>50.8</td>\n",
       "      <td>99.2</td>\n",
       "      <td>103.5</td>\n",
       "      <td>10.76</td>\n",
       "      <td>14.42</td>\n",
       "      <td>23.05</td>\n",
       "      <td>22.48</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>12.8</td>\n",
       "      <td>15.1</td>\n",
       "      <td>47.9</td>\n",
       "      <td>52.9</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.321</td>\n",
       "      <td>57.5</td>\n",
       "      <td>48.4</td>\n",
       "      <td>105.2</td>\n",
       "      <td>91.7</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0.63</td>\n",
       "      <td>25.51</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>52.7</td>\n",
       "      <td>54.1</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.456</td>\n",
       "      <td>0.329</td>\n",
       "      <td>52.1</td>\n",
       "      <td>51.8</td>\n",
       "      <td>111.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>2.99</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>32.05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>16.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>50.7</td>\n",
       "      <td>54.7</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.305</td>\n",
       "      <td>0.414</td>\n",
       "      <td>51.3</td>\n",
       "      <td>63.1</td>\n",
       "      <td>95.1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>23.31</td>\n",
       "      <td>23.67</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>14.5</td>\n",
       "      <td>15.2</td>\n",
       "      <td>55.1</td>\n",
       "      <td>57.2</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.359</td>\n",
       "      <td>0.388</td>\n",
       "      <td>54.5</td>\n",
       "      <td>56.3</td>\n",
       "      <td>94.5</td>\n",
       "      <td>93.2</td>\n",
       "      <td>12.43</td>\n",
       "      <td>8.05</td>\n",
       "      <td>27.45</td>\n",
       "      <td>25.02</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>15.1</td>\n",
       "      <td>14.7</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.9</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.295</td>\n",
       "      <td>0.307</td>\n",
       "      <td>49.8</td>\n",
       "      <td>46.4</td>\n",
       "      <td>99.8</td>\n",
       "      <td>95.8</td>\n",
       "      <td>8.37</td>\n",
       "      <td>9.65</td>\n",
       "      <td>16.25</td>\n",
       "      <td>13.63</td>\n",
       "      <td>...</td>\n",
       "      <td>10.8</td>\n",
       "      <td>9.9</td>\n",
       "      <td>14.3</td>\n",
       "      <td>16.4</td>\n",
       "      <td>49.5</td>\n",
       "      <td>54.7</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.334</td>\n",
       "      <td>51.8</td>\n",
       "      <td>50.6</td>\n",
       "      <td>86.3</td>\n",
       "      <td>88.1</td>\n",
       "      <td>2.99</td>\n",
       "      <td>10.63</td>\n",
       "      <td>32.05</td>\n",
       "      <td>20.80</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>54.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.326</td>\n",
       "      <td>65.3</td>\n",
       "      <td>47.8</td>\n",
       "      <td>95.9</td>\n",
       "      <td>94.9</td>\n",
       "      <td>10.99</td>\n",
       "      <td>9.39</td>\n",
       "      <td>15.51</td>\n",
       "      <td>15.03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>52.6</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.303</td>\n",
       "      <td>47.2</td>\n",
       "      <td>58.9</td>\n",
       "      <td>107.5</td>\n",
       "      <td>92.5</td>\n",
       "      <td>-8.49</td>\n",
       "      <td>12.49</td>\n",
       "      <td>-4.04</td>\n",
       "      <td>28.22</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>13.4</td>\n",
       "      <td>48.4</td>\n",
       "      <td>58.2</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    3PAr_1  3PAr_2  AST%_1  AST%_2  AdjD_1  AdjD_2  AdjEM.1_1  AdjEM.1_2  \\\n",
       "8    0.369   0.433    51.9    52.3    92.1    92.5      10.00       9.33   \n",
       "50   0.315   0.313    52.3    49.3   104.4    95.5      -4.80       8.82   \n",
       "43   0.316   0.376    51.6    63.7    91.4   100.4      10.98       5.17   \n",
       "24   0.383   0.393    65.3    53.6    93.9    96.3       9.00      14.05   \n",
       "0    0.433   0.369    52.3    46.0    92.5   103.4       9.33      -4.49   \n",
       "36   0.381   0.364    61.0    59.1    96.3    92.6       3.64       1.50   \n",
       "54   0.325   0.406    57.3    57.8    89.4    98.4      10.84      10.24   \n",
       "28   0.388   0.451    56.3    52.6    93.2    99.2       8.05      10.76   \n",
       "7    0.403   0.383    49.3    47.5   107.6    96.8      -2.86      11.43   \n",
       "49   0.411   0.361    57.1    57.0    98.7    99.0       7.74      11.73   \n",
       "10   0.328   0.366    59.1    56.4    92.9   100.7      13.08       5.49   \n",
       "8    0.433   0.369    52.3    51.9    92.5    92.1       9.33      10.00   \n",
       "35   0.376   0.349    63.7    45.5   100.4   103.3       5.17      -1.92   \n",
       "48   0.384   0.325    54.1    57.3   100.3    89.4      -3.62      10.84   \n",
       "23   0.359   0.380    54.5    65.3    94.5    95.9      12.43      10.99   \n",
       "2    0.411   0.330    52.7    56.4   105.5    88.0       0.40      11.68   \n",
       "37   0.316   0.422    51.6    52.4    91.4   104.9      10.98      -3.35   \n",
       "21   0.451   0.357    52.6    50.8    99.2   103.5      10.76      14.42   \n",
       "22   0.367   0.321    57.5    48.4   105.2    91.7      -1.68      11.14   \n",
       "45   0.456   0.329    52.1    51.8   111.4    86.3      -0.59       2.99   \n",
       "56   0.305   0.414    51.3    63.1    95.1    95.0       7.13       0.41   \n",
       "29   0.359   0.388    54.5    56.3    94.5    93.2      12.43       8.05   \n",
       "31   0.295   0.307    49.8    46.4    99.8    95.8       8.37       9.65   \n",
       "61   0.329   0.334    51.8    50.6    86.3    88.1       2.99      10.63   \n",
       "16   0.380   0.326    65.3    47.8    95.9    94.9      10.99       9.39   \n",
       "30   0.344   0.303    47.2    58.9   107.5    92.5      -8.49      12.49   \n",
       "\n",
       "    AdjEM_1  AdjEM_2   ...    STL%_1  STL%_2  TOV%_1  TOV%_2  TRB%_1  TRB%_2  \\\n",
       "8     22.99    29.88   ...      10.3    11.0    14.3    15.0    54.7    53.0   \n",
       "50     5.07    20.06   ...       8.7    10.2    15.3    14.1    53.9    52.7   \n",
       "43    27.72    23.63   ...       8.1     7.9    13.4    13.4    53.2    52.4   \n",
       "24    23.12    23.45   ...       7.3    10.8    16.1    12.6    54.9    47.7   \n",
       "0     29.88    -3.86   ...      11.0     9.0    15.0    17.1    53.0    44.5   \n",
       "36    15.57    26.14   ...      11.2     9.4    15.0    13.9    49.7    56.1   \n",
       "54    27.17    19.74   ...      13.8    10.4    14.1    12.3    52.1    48.7   \n",
       "28    25.02    23.05   ...       9.4     9.0    14.7    12.8    52.9    47.9   \n",
       "7      2.04    24.17   ...       8.5     8.7    15.3    14.2    51.8    51.9   \n",
       "49    14.34    16.70   ...       8.0     8.8    16.3    15.7    50.3    54.4   \n",
       "10    24.23    13.45   ...       7.9     9.9    17.1    13.7    56.5    50.3   \n",
       "8     29.88    22.99   ...      11.0    10.3    15.0    14.3    53.0    54.7   \n",
       "35    23.63     1.68   ...       7.9     8.9    13.4    14.9    52.4    54.0   \n",
       "48     9.38    27.17   ...      10.2    13.8    16.8    14.1    50.6    52.1   \n",
       "23    27.45    15.51   ...       9.6     6.3    15.1    18.0    53.0    52.6   \n",
       "2     12.46    24.29   ...       9.3     9.1    12.4    13.7    51.6    52.3   \n",
       "37    27.72     1.91   ...       8.1     8.1    13.4    16.4    53.2    53.1   \n",
       "21    23.05    22.48   ...       9.0    10.5    12.8    15.1    47.9    52.9   \n",
       "22     0.63    25.51   ...       6.8     9.7    18.0    13.4    52.7    54.1   \n",
       "45    -1.19    32.05   ...       7.4     9.6    16.4    14.2    50.7    54.7   \n",
       "56    23.31    23.67   ...       8.1     6.9    14.5    15.2    55.1    57.2   \n",
       "29    27.45    25.02   ...       9.6     9.4    15.1    14.7    53.0    52.9   \n",
       "31    16.25    13.63   ...      10.8     9.9    14.3    16.4    49.5    54.7   \n",
       "61    32.05    20.80   ...       9.6    11.0    14.2    15.5    54.7    50.8   \n",
       "16    15.51    15.03   ...       6.3     9.9    18.0    16.6    52.6    53.0   \n",
       "30    -4.04    28.22   ...      10.2     9.7    14.3    13.4    48.4    58.2   \n",
       "\n",
       "    TS%_1  TS%_2  eFG%_1  eFG%_2  \n",
       "8   0.542  0.612   0.521   0.575  \n",
       "50  0.579  0.566   0.554   0.537  \n",
       "43  0.564  0.618   0.529   0.598  \n",
       "24  0.589  0.572   0.557   0.550  \n",
       "0   0.612  0.536   0.575   0.510  \n",
       "36  0.571  0.579   0.538   0.543  \n",
       "54  0.545  0.573   0.513   0.536  \n",
       "28  0.582  0.601   0.554   0.570  \n",
       "7   0.565  0.588   0.528   0.548  \n",
       "49  0.557  0.553   0.523   0.519  \n",
       "10  0.562  0.560   0.530   0.522  \n",
       "8   0.612  0.542   0.575   0.521  \n",
       "35  0.618  0.521   0.598   0.484  \n",
       "48  0.578  0.545   0.553   0.513  \n",
       "23  0.583  0.563   0.561   0.540  \n",
       "2   0.578  0.548   0.555   0.524  \n",
       "37  0.564  0.557   0.529   0.529  \n",
       "21  0.601  0.583   0.570   0.539  \n",
       "22  0.559  0.543   0.528   0.514  \n",
       "45  0.581  0.596   0.537   0.567  \n",
       "56  0.580  0.602   0.537   0.578  \n",
       "29  0.583  0.582   0.561   0.554  \n",
       "31  0.558  0.526   0.514   0.499  \n",
       "61  0.596  0.518   0.567   0.477  \n",
       "16  0.563  0.544   0.540   0.510  \n",
       "30  0.532  0.550   0.481   0.519  \n",
       "\n",
       "[26 rows x 44 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T02:22:47.188792Z",
     "start_time": "2018-01-18T02:22:47.182114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.57623480e-01,   7.81876146e-01,   3.99929639e-01,\n",
       "         9.93293655e-01,   1.66162119e-04,   7.49443508e-01,\n",
       "         7.08152189e-01,   5.81131725e-01,   9.99867038e-01,\n",
       "         9.43839300e-01,   4.92500493e-01,   1.40034403e-01,\n",
       "         3.44308068e-05,   9.97227627e-01,   4.74489418e-02,\n",
       "         8.44144354e-01,   3.52564787e-05,   8.13510043e-01,\n",
       "         9.99558962e-01,   9.99841631e-01,   3.14210278e-01,\n",
       "         1.73099281e-01,   5.37261155e-02,   1.77038692e-01,\n",
       "         1.45736724e-02,   9.95224125e-01])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T05:13:10.913473Z",
     "start_time": "2018-01-18T05:13:10.904142Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'model.pkl') \n",
    "\n",
    "clf = joblib.load('model.pkl') \n",
    "\n",
    "# s = pickle.dumps(clf)\n",
    "# clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T02:21:10.057337Z",
     "start_time": "2018-01-18T02:21:10.024424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.746\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=n_fold, random_state=0)\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(clf, X, y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T19:02:57.998319Z",
     "start_time": "2018-01-17T19:02:57.765112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEmCAYAAABmnDcLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCR1/H/e45gOdYCYfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a14e71748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ndata=int(len(model.coef_[0])/2)\n",
    "plt.bar(np.arange(ndata)-0.15, model.coef_[0][:ndata],width=0.3)\n",
    "plt.bar(np.arange(ndata)+.15, -model.coef_[0][ndata:],width=0.3)\n",
    "labels=dfClean.keys();\n",
    "plt.xticks(np.arange(ndata), labels, rotation='vertical');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show logistic regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "# logit(X_test.dot(model.coef_.T)).values-model.predict_proba(X_test)[:,1,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAESCAYAAAASQMmzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1411ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xcoord = X_test.dot(model.coef_.T)\n",
    "plt.scatter(xcoord, y_test,color=[1,0,1,.05])\n",
    "plt.scatter(-xcoord, 1-y_test,color=[1,0,1,.05])\n",
    "plt.xlabel('${\\cal Z}=\\sum \\\\beta_i {\\cal X}_i$')\n",
    "plt.ylabel('Outcome')\n",
    "x=np.linspace(-10,10,100)\n",
    "plt.plot(x,logit(x),'r--')\n",
    "plt.xlim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [221, 265]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5dba9120e7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predPythag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [221, 265]"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test,np.round(y_predPythag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43,  37],\n",
       "       [ 23, 118]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75113122171945701"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(X_train,y_train)\n",
    "pred=rf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-0636f2d161fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Log reg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pythag'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scatter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resDF' is not defined"
     ]
    }
   ],
   "source": [
    "resDF.plot(x='Log reg',y='Pythag',kind='scatter')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with teams split by k-means and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 613\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.68515497553\n",
      "10-fold cross validation average accuracy: 0.669\n",
      "01 281\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy using pythagorean win expectation:  0.79359430605\n",
      "10-fold cross validation average accuracy: 0.830\n",
      "02 940\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.76170212766\n",
      "10-fold cross validation average accuracy: 0.754\n",
      "11 388\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.682989690722\n",
      "10-fold cross validation average accuracy: 0.669\n",
      "12 287\n",
      "Accuracy of logistic regression classifier on test set: 0.92\n",
      "Accuracy using pythagorean win expectation:  0.91637630662\n",
      "10-fold cross validation average accuracy: 0.922\n",
      "22 530\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.694339622642\n",
      "10-fold cross validation average accuracy: 0.687\n",
      "\n",
      "0.737416863787\n",
      "0.742020401448\n",
      "00 573\n",
      "Accuracy of logistic regression classifier on test set: 0.72\n",
      "Accuracy using pythagorean win expectation:  0.713787085515\n",
      "10-fold cross validation average accuracy: 0.651\n",
      "01 282\n",
      "Accuracy of logistic regression classifier on test set: 0.81\n",
      "Accuracy using pythagorean win expectation:  0.808510638298\n",
      "10-fold cross validation average accuracy: 0.815\n",
      "02 863\n",
      "Accuracy of logistic regression classifier on test set: 0.78\n",
      "Accuracy using pythagorean win expectation:  0.800695249131\n",
      "10-fold cross validation average accuracy: 0.778\n",
      "11 408\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.671568627451\n",
      "10-fold cross validation average accuracy: 0.670\n",
      "12 380\n",
      "Accuracy of logistic regression classifier on test set: 0.91\n",
      "Accuracy using pythagorean win expectation:  0.910526315789\n",
      "10-fold cross validation average accuracy: 0.920\n",
      "22 526\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.665399239544\n",
      "10-fold cross validation average accuracy: 0.642\n",
      "\n",
      "0.737129063522\n",
      "0.757915567282\n",
      "00 309\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.669902912621\n",
      "10-fold cross validation average accuracy: 0.630\n",
      "01 840\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.789285714286\n",
      "10-fold cross validation average accuracy: 0.771\n",
      "02 282\n",
      "Accuracy of logistic regression classifier on test set: 0.91\n",
      "Accuracy using pythagorean win expectation:  0.918439716312\n",
      "10-fold cross validation average accuracy: 0.899\n",
      "11 644\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.680124223602\n",
      "10-fold cross validation average accuracy: 0.662\n",
      "12 390\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.776923076923\n",
      "10-fold cross validation average accuracy: 0.774\n",
      "22 536\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.67723880597\n",
      "10-fold cross validation average accuracy: 0.693\n",
      "\n",
      "0.731521485249\n",
      "0.744085304898\n",
      "00 586\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.692832764505\n",
      "10-fold cross validation average accuracy: 0.699\n",
      "01 287\n",
      "Accuracy of logistic regression classifier on test set: 0.82\n",
      "Accuracy using pythagorean win expectation:  0.794425087108\n",
      "10-fold cross validation average accuracy: 0.801\n",
      "02 442\n",
      "Accuracy of logistic regression classifier on test set: 0.85\n",
      "Accuracy using pythagorean win expectation:  0.864253393665\n",
      "10-fold cross validation average accuracy: 0.867\n",
      "11 345\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.649275362319\n",
      "10-fold cross validation average accuracy: 0.644\n",
      "12 802\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.779301745636\n",
      "10-fold cross validation average accuracy: 0.788\n",
      "22 543\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.664825046041\n",
      "10-fold cross validation average accuracy: 0.681\n",
      "\n",
      "0.74747637304\n",
      "0.740765391015\n",
      "00 532\n",
      "Accuracy of logistic regression classifier on test set: 0.66\n",
      "Accuracy using pythagorean win expectation:  0.65037593985\n",
      "10-fold cross validation average accuracy: 0.698\n",
      "01 341\n",
      "Accuracy of logistic regression classifier on test set: 0.91\n",
      "Accuracy using pythagorean win expectation:  0.91788856305\n",
      "10-fold cross validation average accuracy: 0.913\n",
      "02 296\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.787162162162\n",
      "10-fold cross validation average accuracy: 0.790\n",
      "11 532\n",
      "Accuracy of logistic regression classifier on test set: 0.66\n",
      "Accuracy using pythagorean win expectation:  0.646616541353\n",
      "10-fold cross validation average accuracy: 0.685\n",
      "12 810\n",
      "Accuracy of logistic regression classifier on test set: 0.80\n",
      "Accuracy using pythagorean win expectation:  0.802469135802\n",
      "10-fold cross validation average accuracy: 0.783\n",
      "22 471\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.709129511677\n",
      "10-fold cross validation average accuracy: 0.658\n",
      "\n",
      "0.746160913262\n",
      "0.744466800805\n",
      "00 536\n",
      "Accuracy of logistic regression classifier on test set: 0.73\n",
      "Accuracy using pythagorean win expectation:  0.722014925373\n",
      "10-fold cross validation average accuracy: 0.694\n",
      "01 715\n",
      "Accuracy of logistic regression classifier on test set: 0.75\n",
      "Accuracy using pythagorean win expectation:  0.74965034965\n",
      "10-fold cross validation average accuracy: 0.770\n",
      "02 513\n",
      "Accuracy of logistic regression classifier on test set: 0.90\n",
      "Accuracy using pythagorean win expectation:  0.906432748538\n",
      "10-fold cross validation average accuracy: 0.913\n",
      "11 293\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.686006825939\n",
      "10-fold cross validation average accuracy: 0.691\n",
      "12 322\n",
      "Accuracy of logistic regression classifier on test set: 0.74\n",
      "Accuracy using pythagorean win expectation:  0.754658385093\n",
      "10-fold cross validation average accuracy: 0.761\n",
      "22 557\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.678635547576\n",
      "10-fold cross validation average accuracy: 0.701\n",
      "\n",
      "0.759121806828\n",
      "0.75272479564\n",
      "00 411\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.698296836983\n",
      "10-fold cross validation average accuracy: 0.662\n",
      "01 787\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.790343074968\n",
      "10-fold cross validation average accuracy: 0.779\n",
      "02 237\n",
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "Accuracy using pythagorean win expectation:  0.940928270042\n",
      "10-fold cross validation average accuracy: 0.936\n",
      "11 663\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.613876319759\n",
      "10-fold cross validation average accuracy: 0.661\n",
      "12 468\n",
      "Accuracy of logistic regression classifier on test set: 0.80\n",
      "Accuracy using pythagorean win expectation:  0.801282051282\n",
      "10-fold cross validation average accuracy: 0.812\n",
      "22 357\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.627450980392\n",
      "10-fold cross validation average accuracy: 0.670\n",
      "\n",
      "0.740584422896\n",
      "0.731440301061\n",
      "00 360\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.716666666667\n",
      "10-fold cross validation average accuracy: 0.652\n",
      "01 184\n",
      "Accuracy of logistic regression classifier on test set: 0.98\n",
      "Accuracy using pythagorean win expectation:  0.994565217391\n",
      "10-fold cross validation average accuracy: 0.960\n",
      "02 501\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy using pythagorean win expectation:  0.834331337325\n",
      "10-fold cross validation average accuracy: 0.823\n",
      "11 368\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.627\n",
      "12 638\n",
      "Accuracy of logistic regression classifier on test set: 0.82\n",
      "Accuracy using pythagorean win expectation:  0.815047021944\n",
      "10-fold cross validation average accuracy: 0.799\n",
      "22 674\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.695845697329\n",
      "10-fold cross validation average accuracy: 0.650\n",
      "\n",
      "0.734948243054\n",
      "0.771009174312\n",
      "00 612\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.62091503268\n",
      "10-fold cross validation average accuracy: 0.644\n",
      "01 431\n",
      "Accuracy of logistic regression classifier on test set: 0.88\n",
      "Accuracy using pythagorean win expectation:  0.867749419954\n",
      "10-fold cross validation average accuracy: 0.845\n",
      "02 646\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.78173374613\n",
      "10-fold cross validation average accuracy: 0.764\n",
      "11 406\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.709359605911\n",
      "10-fold cross validation average accuracy: 0.654\n",
      "12 195\n",
      "Accuracy of logistic regression classifier on test set: 0.93\n",
      "Accuracy using pythagorean win expectation:  0.94358974359\n",
      "10-fold cross validation average accuracy: 0.960\n",
      "22 404\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.660891089109\n",
      "10-fold cross validation average accuracy: 0.656\n",
      "\n",
      "0.731065819635\n",
      "0.741648106904\n",
      "00 499\n",
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "Accuracy using pythagorean win expectation:  0.689378757515\n",
      "10-fold cross validation average accuracy: 0.688\n",
      "01 337\n",
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "Accuracy using pythagorean win expectation:  0.756676557864\n",
      "10-fold cross validation average accuracy: 0.761\n",
      "02 312\n",
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "Accuracy using pythagorean win expectation:  0.942307692308\n",
      "10-fold cross validation average accuracy: 0.937\n",
      "11 362\n",
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "Accuracy using pythagorean win expectation:  0.657458563536\n",
      "10-fold cross validation average accuracy: 0.610\n",
      "12 701\n",
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "Accuracy using pythagorean win expectation:  0.760342368046\n",
      "10-fold cross validation average accuracy: 0.757\n",
      "22 450\n",
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "Accuracy using pythagorean win expectation:  0.673333333333\n",
      "10-fold cross validation average accuracy: 0.656\n",
      "\n",
      "0.728378703589\n",
      "0.739195791056\n",
      "00 326\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.684049079755\n",
      "10-fold cross validation average accuracy: 0.612\n",
      "01 655\n",
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "Accuracy using pythagorean win expectation:  0.795419847328\n",
      "10-fold cross validation average accuracy: 0.757\n",
      "02 208\n",
      "Accuracy of logistic regression classifier on test set: 0.93\n",
      "Accuracy using pythagorean win expectation:  0.932692307692\n",
      "10-fold cross validation average accuracy: 0.928\n",
      "11 533\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.679174484053\n",
      "10-fold cross validation average accuracy: 0.683\n",
      "12 416\n",
      "Accuracy of logistic regression classifier on test set: 0.78\n",
      "Accuracy using pythagorean win expectation:  0.762019230769\n",
      "10-fold cross validation average accuracy: 0.803\n",
      "22 481\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.656964656965\n",
      "10-fold cross validation average accuracy: 0.703\n",
      "\n",
      "0.734598731354\n",
      "0.738067964872\n",
      "00 482\n",
      "Accuracy of logistic regression classifier on test set: 0.69\n",
      "Accuracy using pythagorean win expectation:  0.676348547718\n",
      "10-fold cross validation average accuracy: 0.661\n",
      "01 517\n",
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "Accuracy using pythagorean win expectation:  0.762088974855\n",
      "10-fold cross validation average accuracy: 0.779\n",
      "02 351\n",
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "Accuracy using pythagorean win expectation:  0.786324786325\n",
      "10-fold cross validation average accuracy: 0.766\n",
      "11 355\n",
      "Accuracy of logistic regression classifier on test set: 0.73\n",
      "Accuracy using pythagorean win expectation:  0.732394366197\n",
      "10-fold cross validation average accuracy: 0.683\n",
      "12 280\n",
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "Accuracy using pythagorean win expectation:  0.946428571429\n",
      "10-fold cross validation average accuracy: 0.925\n",
      "22 530\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.705660377358\n",
      "10-fold cross validation average accuracy: 0.670\n",
      "\n",
      "0.734261502166\n",
      "0.753479125249\n",
      "00 571\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.661996497373\n",
      "10-fold cross validation average accuracy: 0.625\n",
      "01 518\n",
      "Accuracy of logistic regression classifier on test set: 0.75\n",
      "Accuracy using pythagorean win expectation:  0.764478764479\n",
      "10-fold cross validation average accuracy: 0.774\n",
      "02 562\n",
      "Accuracy of logistic regression classifier on test set: 0.83\n",
      "Accuracy using pythagorean win expectation:  0.834519572954\n",
      "10-fold cross validation average accuracy: 0.825\n",
      "11 185\n",
      "Accuracy of logistic regression classifier on test set: 0.70\n",
      "Accuracy using pythagorean win expectation:  0.708108108108\n",
      "10-fold cross validation average accuracy: 0.674\n",
      "12 139\n",
      "Accuracy of logistic regression classifier on test set: 0.96\n",
      "Accuracy using pythagorean win expectation:  0.956834532374\n",
      "10-fold cross validation average accuracy: 0.932\n",
      "22 537\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.683426443203\n",
      "10-fold cross validation average accuracy: 0.673\n",
      "\n",
      "0.731158697519\n",
      "0.74601910828\n",
      "00 259\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "Accuracy using pythagorean win expectation:  0.644787644788\n",
      "10-fold cross validation average accuracy: 0.669\n",
      "01 581\n",
      "Accuracy of logistic regression classifier on test set: 0.81\n",
      "Accuracy using pythagorean win expectation:  0.822719449225\n",
      "10-fold cross validation average accuracy: 0.837\n",
      "02 95\n",
      "Accuracy of logistic regression classifier on test set: 0.96\n",
      "Accuracy using pythagorean win expectation:  0.989473684211\n",
      "10-fold cross validation average accuracy: 0.918\n",
      "11 706\n",
      "Accuracy of logistic regression classifier on test set: 0.66\n",
      "Accuracy using pythagorean win expectation:  0.661473087819\n",
      "10-fold cross validation average accuracy: 0.687\n",
      "12 437\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy using pythagorean win expectation:  0.844393592677\n",
      "10-fold cross validation average accuracy: 0.815\n",
      "22 415\n",
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "Accuracy using pythagorean win expectation:  0.698795180723\n",
      "10-fold cross validation average accuracy: 0.707\n",
      "\n",
      "0.754569502728\n",
      "0.748094665062\n"
     ]
    }
   ],
   "source": [
    "resultList=[]\n",
    "for yr in np.arange(2017,2003,-1):\n",
    "    year=str(yr)\n",
    "    teamType=pd.read_csv('data/team_type_'+year+'.csv')\n",
    "    teamTypeDict={}\n",
    "    for r in teamType.values:\n",
    "        teamTypeDict[r[0]]=r[1]\n",
    "\n",
    "    df=pd.read_csv('data/games/all_games_'+year+'.csv',index_col=0).dropna(axis=1)\n",
    "    # df=pd.read_csv('data/games/tourn_games_2016.csv',index_col=0)\n",
    "    # y=df['outcome']\n",
    "    dropLabels=['School_1','Conf_1','wpct_1','Rank_1','WL_1','sched_url_1',\\\n",
    "                'School_2','Conf_2','wpct_2','Rank_2','WL_2','sched_url_2', 'outcome']\n",
    "\n",
    "    y=df['outcome']\n",
    "\n",
    "\n",
    "    types=[]\n",
    "    for r in df.itertuples():\n",
    "\n",
    "        t1=teamTypeDict[r.School_1]\n",
    "        t2=teamTypeDict[r.School_2]\n",
    "\n",
    "        types.append(str(min(t1,t2))+str(max(t1,t2)))\n",
    "\n",
    "    df['types']=types\n",
    "\n",
    "    runsum=0\n",
    "    numsum=0\n",
    "    pythsum=0\n",
    "\n",
    "    nlab=3\n",
    "    for t1lab in range(nlab):\n",
    "        for t2lab in np.arange(t1lab,nlab):\n",
    "            lab = str(t1lab)+str(t2lab)\n",
    "\n",
    "            dfSel = df[(df.types==lab)]\n",
    "            dropLabels.append('types')\n",
    "\n",
    "            X=dfSel.drop(dropLabels,axis=1)\n",
    "            y=dfSel['outcome']\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X_train, y_train)\n",
    "\n",
    "            y_pred=logreg.predict(X_test)\n",
    "            print(lab,len(y_pred))\n",
    "            print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "            y_predPythag=pythagGame(X_test)\n",
    "\n",
    "            print('Accuracy using pythagorean win expectation: ',(np.round(y_predPythag)==y_test).sum()/len(y_test))\n",
    "\n",
    "\n",
    "            kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "            modelCV = LogisticRegression()\n",
    "            scoring = 'accuracy'\n",
    "            results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "            print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))    \n",
    "\n",
    "            runsum+= len(y_pred) * results.mean() #logreg.score(X_test, y_test)\n",
    "            numsum+= len(y_pred)\n",
    "            pythsum+= len(y_pred) *(np.round(y_predPythag)==y_test).sum()/len(y_test)\n",
    "\n",
    "\n",
    "\n",
    "    print ()\n",
    "    print (runsum/numsum)\n",
    "    print (pythsum/numsum)\n",
    "    \n",
    "    resultList.append([year,(runsum/numsum),pythsum/numsum])\n",
    "    \n",
    "resDFKM=pd.DataFrame(resultList,columns=['year','Log reg','Pythag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year       1.440858e+54\n",
       "Log reg    7.391709e-01\n",
       "Pythag     7.464952e-01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resDFKM.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.70\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "# logreg = LogisticRegression(C=1,penalty='l1',tol=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred=logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pythag']=pythagGame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using pythagorean win expectation:  0.739459815547\n"
     ]
    }
   ],
   "source": [
    "y_predPythag=pythagGame(X_test)\n",
    "\n",
    "print('Accuracy using pythagorean win expectation: ',(np.round(y_predPythag)==y_test).sum()/len(y_test))\n",
    "# print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
