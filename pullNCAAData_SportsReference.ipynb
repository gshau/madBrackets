{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Sports Reference for CBB data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T20:47:41.963007Z",
     "start_time": "2018-01-17T20:47:40.751419Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "head_names = ['School', 'W%', 'SRS', 'SOS', 'TmPts', 'OppPts', \n",
    "              'Pace', 'ORtg', 'FTr', '3PAr', 'TS%', 'TRB%', 'AST%', \n",
    "              'STL%', 'BLK%', 'eFG%', 'TOV%', 'ORB%', 'FT/FGA', 'sched_url']\n",
    "\n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "\n",
    "\n",
    "years = ['2017', '2016', '2015', '2014', '2013', '2012', '2011', '2010']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T20:47:41.981035Z",
     "start_time": "2018-01-17T20:47:41.965515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017',\n",
       " '2016',\n",
       " '2015',\n",
       " '2014',\n",
       " '2013',\n",
       " '2012',\n",
       " '2011',\n",
       " '2010',\n",
       " '2009',\n",
       " '2008',\n",
       " '2007',\n",
       " '2006',\n",
       " '2005',\n",
       " '2004',\n",
       " '2003',\n",
       " '2002',\n",
       " '2001',\n",
       " '2000',\n",
       " '1999',\n",
       " '1998',\n",
       " '1997',\n",
       " '1996',\n",
       " '1995',\n",
       " '1994',\n",
       " '1993',\n",
       " '1992',\n",
       " '1991',\n",
       " '1990',\n",
       " '1989',\n",
       " '1988',\n",
       " '1987',\n",
       " '1986',\n",
       " '1985']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [str(yr) for yr in np.arange(2017,1984,-1)]\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T20:50:52.429889Z",
     "start_time": "2018-01-17T20:47:47.615420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the data for 2017\n",
      "Compiling the data for 2016\n",
      "Compiling the data for 2015\n",
      "Compiling the data for 2014\n",
      "Compiling the data for 2013\n",
      "Compiling the data for 2012\n",
      "Compiling the data for 2011\n",
      "Compiling the data for 2010\n",
      "Compiling the data for 2009\n",
      "Compiling the data for 2008\n",
      "Compiling the data for 2007\n",
      "Compiling the data for 2006\n",
      "Compiling the data for 2005\n",
      "Compiling the data for 2004\n",
      "Compiling the data for 2003\n",
      "Compiling the data for 2002\n",
      "Compiling the data for 2001\n",
      "Compiling the data for 2000\n",
      "Compiling the data for 1999\n",
      "Compiling the data for 1998\n",
      "Compiling the data for 1997\n",
      "Compiling the data for 1996\n",
      "Compiling the data for 1995\n",
      "Compiling the data for 1994\n",
      "Compiling the data for 1993\n",
      "Compiling the data for 1992\n",
      "Compiling the data for 1991\n",
      "Compiling the data for 1990\n",
      "Compiling the data for 1989\n",
      "Compiling the data for 1988\n",
      "Compiling the data for 1987\n",
      "Compiling the data for 1986\n",
      "Compiling the data for 1985\n"
     ]
    }
   ],
   "source": [
    "decode = \"utf-8\"\n",
    "for year in years:\n",
    "\n",
    "    print(\"Compiling the data for \" + year)\n",
    "\n",
    "    url = \"http://www.sports-reference.com/cbb/seasons/\"+year+\"-advanced-school-stats.html\"\n",
    "    r = requests.get(url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, \"html5lib\")\n",
    "\n",
    "    team_names = soup.findAll(\"td\", {\"data-stat\": \"school_name\"})\n",
    "    team_schedurl = [node.find('a')['href'][:-5]+'-schedule.html' for node in team_names]\n",
    "    team_names = [node.getText().encode(decode) for node in team_names]\n",
    "#     team_names = [node.lower().replace(\" *\", \"\").replace(\" \", \"-\") for node in team_names]\n",
    "    team_names=[node.lower().replace(b\" *\",b\" \").replace(b\" \", b\"-\") for node in team_names]\n",
    "    team_names=[node.split(b'\\xc2')[0].decode('ascii') for node in team_names]\n",
    "\n",
    "    full_names=soup.findAll(\"td\", {\"data-stat\": \"school_name\"})\n",
    "    full_names=[node.getText().encode(decode) for node in full_names]\n",
    "    full_names=[node.replace(b\" *\",b\" \") for node in full_names]\n",
    "    full_names=[node.split(b'\\xc2')[0].decode('ascii') for node in full_names]\n",
    "    \n",
    "    team_WLpct = soup.findAll(\"td\", {\"data-stat\": \"win_loss_pct\"})\n",
    "    team_WLpct = [node.getText().encode(decode) for node in team_WLpct]\n",
    "    team_WLpct = [num(stat) if stat else 'nan'  for stat in team_WLpct]\n",
    "\n",
    "    team_srs = soup.findAll(\"td\", {\"data-stat\": \"srs\"})\n",
    "    team_srs = [node.getText().encode(decode) for node in team_srs]\n",
    "    team_srs = [num(stat)  if stat else 'nan' for stat in team_srs]\n",
    "\n",
    "    team_sos = soup.findAll(\"td\", {\"data-stat\": \"sos\"})\n",
    "    team_sos = [node.getText().encode(decode) for node in team_sos]\n",
    "    team_sos = [num(stat)  if stat else 'nan' for stat in team_sos]\n",
    "\n",
    "    team_TmPts = soup.findAll(\"td\", {\"data-stat\": \"pts\"})\n",
    "    team_TmPts = [node.getText().encode(decode) for node in team_TmPts]\n",
    "    team_TmPts = [num(stat)  if stat else 'nan' for stat in team_TmPts]\n",
    "\n",
    "    team_OppPts = soup.findAll(\"td\", {\"data-stat\": \"opp_pts\"})\n",
    "    team_OppPts = [node.getText().encode(decode) for node in team_OppPts]\n",
    "    team_OppPts = [num(stat)  if stat else 'nan' for stat in team_OppPts]\n",
    "\n",
    "    team_pace = soup.findAll(\"td\", {\"data-stat\": \"pace\"})\n",
    "    team_pace = [node.getText().encode(decode) for node in team_pace]\n",
    "    team_pace = [num(stat)  if stat else 'nan' for stat in team_pace]\n",
    "\n",
    "    team_ORtg = soup.findAll(\"td\", {\"data-stat\": \"off_rtg\"})\n",
    "    team_ORtg = [node.getText().encode(decode) for node in team_ORtg]\n",
    "    team_ORtg = [num(stat)  if stat else 'nan' for stat in team_ORtg]\n",
    "\n",
    "    team_FTr = soup.findAll(\"td\", {\"data-stat\": \"fta_per_fga_pct\"})\n",
    "    team_FTr = [node.getText().encode(decode) for node in team_FTr]\n",
    "    team_FTr = [num(stat)  if stat else 'nan' for stat in team_FTr]\n",
    "\n",
    "    team_3Ar = soup.findAll(\"td\", {\"data-stat\": \"fg3a_per_fga_pct\"})\n",
    "    team_3Ar = [node.getText().encode(decode) for node in team_3Ar]\n",
    "    team_3Ar = [num(stat)  if stat else 'nan' for stat in team_3Ar]\n",
    "\n",
    "    team_TSpct = soup.findAll(\"td\", {\"data-stat\": \"ts_pct\"})\n",
    "    team_TSpct = [node.getText().encode(decode) for node in team_TSpct]\n",
    "    team_TSpct = [num(stat)  if stat else 'nan' for stat in team_TSpct]\n",
    "\n",
    "    team_TRBpct = soup.findAll(\"td\", {\"data-stat\": \"trb_pct\"})\n",
    "    team_TRBpct = [node.getText().encode(decode) for node in team_TRBpct]\n",
    "    team_TRBpct = [num(stat)  if stat else 'nan' for stat in team_TRBpct]\n",
    "\n",
    "    team_ASTpct = soup.findAll(\"td\", {\"data-stat\": \"ast_pct\"})\n",
    "    team_ASTpct = [node.getText().encode(decode) for node in team_ASTpct]\n",
    "    team_ASTpct = [num(stat)  if stat else 'nan' for stat in team_ASTpct]\n",
    "\n",
    "    team_STLpct = soup.findAll(\"td\", {\"data-stat\": \"stl_pct\"})\n",
    "    team_STLpct = [node.getText().encode(decode) for node in team_STLpct]\n",
    "    team_STLpct = [num(stat)  if stat else 'nan' for stat in team_STLpct]\n",
    "\n",
    "    team_BLKpct = soup.findAll(\"td\", {\"data-stat\": \"blk_pct\"})\n",
    "    team_BLKpct = [node.getText().encode(decode) for node in team_BLKpct]\n",
    "    team_BLKpct = [num(stat)  if stat else 'nan' for stat in team_BLKpct]\n",
    "\n",
    "    team_eFGpct = soup.findAll(\"td\", {\"data-stat\": \"efg_pct\"})\n",
    "    team_eFGpct = [node.getText().encode(decode) for node in team_eFGpct]\n",
    "    team_eFGpct = [num(stat)  if stat else 'nan' for stat in team_eFGpct]\n",
    "\n",
    "    team_TOVpct = soup.findAll(\"td\", {\"data-stat\": \"tov_pct\"})\n",
    "    team_TOVpct = [node.getText().encode(decode) for node in team_TOVpct]\n",
    "    team_TOVpct = [num(stat)  if stat else 'nan' for stat in team_TOVpct]\n",
    "\n",
    "    team_ORBpct = soup.findAll(\"td\", {\"data-stat\": \"orb_pct\"})\n",
    "    team_ORBpct = [node.getText().encode(decode) for node in team_ORBpct]\n",
    "    team_ORBpct = [num(stat)  if stat else 'nan' for stat in team_ORBpct]\n",
    "\n",
    "    team_FTr = soup.findAll(\"td\", {\"data-stat\": \"ft_rate\"})\n",
    "    team_FTr = [node.getText().encode(decode) for node in team_FTr]\n",
    "    team_FTr = [num(stat)  if stat else 'nan' for stat in team_FTr]\n",
    "\n",
    "    stats_list = [team_names, team_WLpct, team_srs, team_sos, team_TmPts,\n",
    "             team_OppPts, team_pace, team_ORtg, team_FTr, team_3Ar,\n",
    "             team_TSpct, team_TRBpct, team_ASTpct, team_STLpct, \n",
    "             team_BLKpct, team_eFGpct, team_TOVpct, team_ORBpct, \n",
    "             team_FTr, team_schedurl]\n",
    "\n",
    "    team_dict = {}\n",
    "    for i, head in enumerate(head_names):\n",
    "        team_dict[head] = stats_list[i]\n",
    "\n",
    "    team_data = pd.DataFrame(team_dict)\n",
    "\n",
    "    team_data = team_data[head_names]\n",
    "\n",
    "    \n",
    "    team_data['name']=[t.split('/')[3] for t in team_schedurl]\n",
    "    team_data['fullName']=full_names\n",
    "    \n",
    "    team_data.to_csv(\"data/team/team_stats_\"+year+\".csv\", index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
